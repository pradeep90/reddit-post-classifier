Kaggle guy's results:
RUNNING_KAGGLE_KERNEL == True
precision@1 = 0.610528134254689
precision@3 = 0.7573692003948668
precision@5 = 0.8067670286278381

RUNNING_KAGGLE_KERNEL == False
precision@1 = 0.7292102665350444
precision@3 = 0.8512240868706812
precision@5 = 0.8861500493583415

NBC (200k points)
precision@1 = 0.5346
precision@3 = 0.68945
precision@5 = 0.747975

LogisticRegression (100k points; 47m to train; 1013 rounds of convergence @ 2.78s per round):
precision@1 = 0.50875
precision@3 = 0.6624
precision@5 = 0.7147

LogisticRegression (1k points; 1.5m to train; 1013 rounds of convergence @ 11 rounds per second):
precision@1 = 0.0
precision@3 = 0.005
precision@5 = 0.005

LogisticRegression (1k points; 1.5m to train):
Start: Fri Apr 12 17:07:33 EDT 2019
precision@1 = 0.0
precision@3 = 0.005
precision@5 = 0.005
End: Fri Apr 12 17:08:58 EDT 2019

Start: Fri Apr 12 18:31:52 EDT 2019
model LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='warn',
          n_jobs=None, penalty='l2', random_state=None, solver='sag',
          tol=0.0001, verbose=0, warm_start=False)
DATASET_SIZE: 100
precision@1 = 0.0
precision@3 = 0.05
precision@5 = 0.05
End: Fri Apr 12 18:32:42 EDT 2019


Start: Fri Apr 12 18:51:39 EDT 2019

Start: Fri Apr 12 19:06:30 EDT 2019
model LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='warn',
          n_jobs=None, penalty='l2', random_state=None, solver='sag',
          tol=0.0001, verbose=0, warm_start=False)
precision@1 = 0.0
precision@3 = 0.05
precision@5 = 0.05
End: Fri Apr 12 19:06:52 EDT 2019


Start: Fri Apr 12 19:11:17 EDT 2019
model LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='warn',
          n_jobs=None, penalty='l2', random_state=None, solver='sag',
          tol=0.0001, verbose=0, warm_start=False)
precision@1 = 0.00125
precision@3 = 0.00375
precision@5 = 0.005
End: Fri Apr 12 19:16:21 EDT 2019


Start: Fri Apr 12 19:38:31 EDT 2019
model LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='warn',
          n_jobs=None, penalty='l2', random_state=None, solver='sag',
          tol=0.0001, verbose=0, warm_start=False)
precision@1 = 0.00125
precision@3 = 0.0025
precision@5 = 0.005
End: Fri Apr 12 19:43:46 EDT 2019


Start: Fri Apr 12 19:44:53 EDT 2019
model LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='warn',
          n_jobs=None, penalty='l2', random_state=None, solver='sag',
          tol=0.0001, verbose=0, warm_start=False)
precision@1 = 0.0555
precision@3 = 0.0975
precision@5 = 0.12
End: Fri Apr 12 19:59:17 EDT 2019


Start: Fri Apr 12 20:17:05 EDT 2019
model LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='warn',
          n_jobs=None, penalty='l2', random_state=None, solver='sag',
          tol=0.0001, verbose=0, warm_start=False)
precision@1 = 0.0
precision@3 = 0.0
precision@5 = 0.05
End: Fri Apr 12 20:17:22 EDT 2019


Start: Sat Apr 13 16:46:22 EDT 2019
End: Sat Apr 13 16:46:22 EDT 2019


Start: Sat Apr 13 16:46:58 EDT 2019
End: Sat Apr 13 16:46:58 EDT 2019


Start: Sat Apr 13 16:47:12 EDT 2019
End: Sat Apr 13 16:47:12 EDT 2019


Start: Sat Apr 13 16:47:23 EDT 2019
End: Sat Apr 13 16:47:24 EDT 2019


Start: Sat Apr 13 16:48:53 EDT 2019
End: Sat Apr 13 16:48:53 EDT 2019


Start: Sat Apr 13 16:48:55 EDT 2019
End: Sat Apr 13 16:48:56 EDT 2019


Start: Sat Apr 13 16:51:01 EDT 2019
Indexing word vectors.
End: Sat Apr 13 16:51:12 EDT 2019


Start: Sat Apr 13 16:51:35 EDT 2019
End: Sat Apr 13 16:51:35 EDT 2019


Start: Sat Apr 13 16:51:40 EDT 2019
Indexing word vectors.
End: Sat Apr 13 16:51:48 EDT 2019


Start: Sat Apr 13 16:52:27 EDT 2019
Indexing word vectors.
Found 400000 word vectors.
Processing text dataset
Found 0 texts.
Found 0 unique tokens.
End: Sat Apr 13 16:52:44 EDT 2019


Start: Sat Apr 13 16:53:13 EDT 2019
Indexing word vectors.
Found 400000 word vectors.
Processing text dataset
Found 19997 texts.
Found 174074 unique tokens.
Shape of data tensor: (19997, 10)
Shape of label tensor: (19997, 20)
Preparing embedding matrix.
End: Sat Apr 13 16:53:50 EDT 2019


Start: Sat Apr 13 16:55:35 EDT 2019
Indexing word vectors.
Found 400000 word vectors.
Processing text dataset
Found 19997 texts.
Found 174074 unique tokens.
Shape of data tensor: (19997, 10)
Shape of label tensor: (19997, 20)
Preparing embedding matrix.
Training model.
End: Sat Apr 13 16:56:05 EDT 2019


Start: Sat Apr 13 16:56:24 EDT 2019
Indexing word vectors.
Found 400000 word vectors.
Processing text dataset
Found 19997 texts.
Found 174074 unique tokens.
Shape of data tensor: (19997, 100)
Shape of label tensor: (19997, 20)
Preparing embedding matrix.
Training model.
End: Sat Apr 13 16:56:54 EDT 2019


Start: Sat Apr 13 16:57:08 EDT 2019
Keras CNN tutorial.
Indexing word vectors.
Found 400000 word vectors.
Processing text dataset
Found 19997 texts.
Found 174074 unique tokens.
Shape of data tensor: (19997, 1000)
Shape of label tensor: (19997, 20)
Preparing embedding matrix.
Training model.
Train on 15998 samples, validate on 3999 samples
15998/15998 [==============================] - 46s 3ms/step - loss: 2.4178 - acc: 0.2055 - val_loss: 1.8238 - val_acc: 0.3711
15998/15998 [==============================] - 44s 3ms/step - loss: 1.5794 - acc: 0.4526 - val_loss: 1.4371 - val_acc: 0.4846
15998/15998 [==============================] - 45s 3ms/step - loss: 1.2057 - acc: 0.5835 - val_loss: 1.2670 - val_acc: 0.5659
15998/15998 [==============================] - 45s 3ms/step - loss: 0.9972 - acc: 0.6603 - val_loss: 1.1051 - val_acc: 0.6409
15998/15998 [==============================] - 45s 3ms/step - loss: 0.8347 - acc: 0.7136 - val_loss: 0.9480 - val_acc: 0.6762
15998/15998 [==============================] - 45s 3ms/step - loss: 0.7123 - acc: 0.7556 - val_loss: 0.9698 - val_acc: 0.6822
15998/15998 [==============================] - 45s 3ms/step - loss: 0.6015 - acc: 0.7943 - val_loss: 0.8732 - val_acc: 0.7217
15998/15998 [==============================] - 45s 3ms/step - loss: 0.5115 - acc: 0.8267 - val_loss: 0.9229 - val_acc: 0.7077
15998/15998 [==============================] - 45s 3ms/step - loss: 0.4382 - acc: 0.8520 - val_loss: 1.0000 - val_acc: 0.7082
15998/15998 [==============================] - 45s 3ms/step - loss: 0.3695 - acc: 0.8759 - val_loss: 0.9788 - val_acc: 0.7224
End: Sat Apr 13 17:05:09 EDT 2019

Start: Sat Apr 13 17:19:13 EDT 2019
Indexing word vectors.
Found 400000 word vectors.
Processing text dataset
Found 19997 texts.
Found 174074 unique tokens.
End: Sat Apr 13 17:19:43 EDT 2019


Start: Sat Apr 13 17:20:24 EDT 2019
Indexing word vectors.
Found 400000 word vectors.
End: Sat Apr 13 17:20:39 EDT 2019


Start: Sat Apr 13 17:20:47 EDT 2019
Indexing word vectors.
Found 400000 word vectors.
Processing text dataset
Found 19997 texts.
Found 174074 unique tokens.
Shape of data tensor: (19997, 1000)
Shape of label tensor: (19997, 20)
Preparing embedding matrix.
Training model.
End: Sat Apr 13 17:21:18 EDT 2019


Start: Sat Apr 13 17:22:09 EDT 2019
Indexing word vectors.
Found 400000 word vectors.
Processing text dataset
Found 19997 texts.
Found 174074 unique tokens.
Shape of data tensor: (19997, 1000)
Shape of label tensor: (19997, 20)
Preparing embedding matrix.
Training model.
Train on 15998 samples, validate on 3999 samples
Epoch 1/1

15998/15998 [==============================] - 47s 3ms/step - loss: 2.4014 - acc: 0.2119 - val_loss: 1.8751 - val_acc: 0.3316
End: Sat Apr 13 17:23:27 EDT 2019


Start: Sat Apr 13 17:25:19 EDT 2019
Indexing word vectors.
Processing text dataset
Found 19997 texts.
{'alt.atheism': 0, 'comp.graphics': 1, 'comp.os.ms-windows.misc': 2, 'comp.sys.ibm.pc.hardware': 3, 'comp.sys.mac.hardware': 4, 'comp.windows.x': 5, 'misc.forsale': 6, 'rec.autos': 7, 'rec.motorcycles': 8, 'rec.sport.baseball': 9, 'rec.sport.hockey': 10, 'sci.crypt': 11, 'sci.electronics': 12, 'sci.med': 13, 'sci.space': 14, 'soc.religion.christian': 15, 'talk.politics.guns': 16, 'talk.politics.mideast': 17, 'talk.politics.misc': 18, 'talk.religion.misc': 19}
End: Sat Apr 13 17:25:26 EDT 2019


Start: Sat Apr 13 17:30:52 EDT 2019
Indexing word vectors.
Processing text dataset
Found 19997 texts.
Found 400000 word vectors.
Found 174074 unique tokens.
Shape of data tensor: (19997, 1000)
Shape of label tensor: (19997, 20)
Preparing embedding matrix.
Training model.
Train on 15998 samples, validate on 3999 samples
Epoch 1/1

End: Sat Apr 13 17:32:11 EDT 2019


Start: Sat Apr 13 17:46:13 EDT 2019
model LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='warn',
          n_jobs=None, penalty='l2', random_state=None, solver='sag',
          tol=0.0001, verbose=0, warm_start=False)
precision@1 = 0.0
precision@3 = 0.0
precision@5 = 0.05
End: Sat Apr 13 17:46:30 EDT 2019

Start: Sat Apr 13 17:59:22 EDT 2019
Indexing word vectors.
Processing text dataset
Found 19997 texts.
End: Sat Apr 13 17:59:30 EDT 2019


Start: Sat Apr 13 18:00:01 EDT 2019
Indexing word vectors.
Processing text dataset
Found 19997 texts.
{'alt.atheism': 0, 'comp.graphics': 1, 'comp.os.ms-windows.misc': 2, 'comp.sys.ibm.pc.hardware': 3, 'comp.sys.mac.hardware': 4, 'comp.windows.x': 5, 'misc.forsale': 6, 'rec.autos': 7, 'rec.motorcycles': 8, 'rec.sport.baseball': 9, 'rec.sport.hockey': 10, 'sci.crypt': 11, 'sci.electronics': 12, 'sci.med': 13, 'sci.space': 14, 'soc.religion.christian': 15, 'talk.politics.guns': 16, 'talk.politics.mideast': 17, 'talk.politics.misc': 18, 'talk.religion.misc': 19}
{0: 999, 1: 1999, 2: 2999, 3: 3999, 4: 4999, 5: 5999, 6: 6999, 7: 7999, 8: 8999, 9: 9999, 10: 10999, 11: 11999, 12: 12999, 13: 13999, 14: 14999, 15: 15996, 16: 16996, 17: 17996, 18: 18996, 19: 19996}
End: Sat Apr 13 18:00:09 EDT 2019


Start: Sat Apr 13 18:02:13 EDT 2019
Indexing word vectors.
Processing text dataset
Found 19997 texts.
{'alt.atheism': 0, 'comp.graphics': 1, 'comp.os.ms-windows.misc': 2, 'comp.sys.ibm.pc.hardware': 3, 'comp.sys.mac.hardware': 4, 'comp.windows.x': 5, 'misc.forsale': 6, 'rec.autos': 7, 'rec.motorcycles': 8, 'rec.sport.baseball': 9, 'rec.sport.hockey': 10, 'sci.crypt': 11, 'sci.electronics': 12, 'sci.med': 13, 'sci.space': 14, 'soc.religion.christian': 15, 'talk.politics.guns': 16, 'talk.politics.mideast': 17, 'talk.politics.misc': 18, 'talk.religion.misc': 19}
{0: 999, 1: 1999, 2: 2999, 3: 3999, 4: 4999, 5: 5999, 6: 6999, 7: 7999, 8: 8999, 9: 9999, 10: 10999, 11: 11999, 12: 12999, 13: 13999, 14: 14999, 15: 15996, 16: 16996, 17: 17996, 18: 18996, 19: 19996}
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
End: Sat Apr 13 18:02:21 EDT 2019


Start: Sat Apr 13 18:03:32 EDT 2019
Indexing word vectors.
Processing text dataset
Found 19997 texts.
{'alt.atheism': 0, 'comp.graphics': 1, 'comp.os.ms-windows.misc': 2, 'comp.sys.ibm.pc.hardware': 3, 'comp.sys.mac.hardware': 4, 'comp.windows.x': 5, 'misc.forsale': 6, 'rec.autos': 7, 'rec.motorcycles': 8, 'rec.sport.baseball': 9, 'rec.sport.hockey': 10, 'sci.crypt': 11, 'sci.electronics': 12, 'sci.med': 13, 'sci.space': 14, 'soc.religion.christian': 15, 'talk.politics.guns': 16, 'talk.politics.mideast': 17, 'talk.politics.misc': 18, 'talk.religion.misc': 19}
{0: 999, 1: 1999, 2: 2999, 3: 3999, 4: 4999, 5: 5999, 6: 6999, 7: 7999, 8: 8999, 9: 9999, 10: 10999, 11: 11999, 12: 12999, 13: 13999, 14: 14999, 15: 15996, 16: 16996, 17: 17996, 18: 18996, 19: 19996}
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19]
End: Sat Apr 13 18:03:40 EDT 2019


Start: Sat Apr 13 18:06:34 EDT 2019
Indexing word vectors.
Processing text dataset
Found 19997 texts.
{'alt.atheism': 0, 'comp.graphics': 1, 'comp.os.ms-windows.misc': 2, 'comp.sys.ibm.pc.hardware': 3, 'comp.sys.mac.hardware': 4, 'comp.windows.x': 5, 'misc.forsale': 6, 'rec.autos': 7, 'rec.motorcycles': 8, 'rec.sport.baseball': 9, 'rec.sport.hockey': 10, 'sci.crypt': 11, 'sci.electronics': 12, 'sci.med': 13, 'sci.space': 14, 'soc.religion.christian': 15, 'talk.politics.guns': 16, 'talk.politics.mideast': 17, 'talk.politics.misc': 18, 'talk.religion.misc': 19}
{'alt.atheism': 0, 'comp.graphics': 1, 'comp.os.ms-windows.misc': 2, 'comp.sys.ibm.pc.hardware': 3, 'comp.sys.mac.hardware': 4, 'comp.windows.x': 5, 'misc.forsale': 6, 'rec.autos': 7, 'rec.motorcycles': 8, 'rec.sport.baseball': 9, 'rec.sport.hockey': 10, 'sci.crypt': 11, 'sci.electronics': 12, 'sci.med': 13, 'sci.space': 14, 'soc.religion.christian': 15, 'talk.politics.guns': 16, 'talk.politics.mideast': 17, 'talk.politics.misc': 18, 'talk.religion.misc': 19}
['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']
End: Sat Apr 13 18:06:42 EDT 2019


Start: Sat Apr 13 18:13:35 EDT 2019
Indexing word vectors.
Processing text dataset
Found 19997 texts.
End: Sat Apr 13 18:13:42 EDT 2019


Start: Sat Apr 13 18:16:46 EDT 2019
Indexing word vectors.
Processing text dataset
Found 19997 texts.
End: Sat Apr 13 18:16:54 EDT 2019


Start: Sat Apr 13 18:17:14 EDT 2019
Indexing word vectors.
Processing text dataset
Found 19997 texts.
End: Sat Apr 13 18:17:21 EDT 2019


Start: Sat Apr 13 18:18:26 EDT 2019
Indexing word vectors.
Processing text dataset
Found 19997 texts.
End: Sat Apr 13 18:18:33 EDT 2019


Start: Sat Apr 13 18:25:37 EDT 2019
Indexing word vectors.
Processing text dataset
Found 19997 texts.
Found 400000 word vectors.
Found 174074 unique tokens.
Shape of data tensor: (19997, 1000)
Shape of label tensor: (19997, 20)
Preparing embedding matrix.
Training model.
Train on 15998 samples, validate on 3999 samples
Epoch 1/1

15998/15998 [==============================] - 49s 3ms/step - loss: 2.3845 - acc: 0.2141 - val_loss: 1.6949 - val_acc: 0.4064
End: Sat Apr 13 18:26:57 EDT 2019


Start: Sat Apr 13 18:32:22 EDT 2019
Indexing word vectors.
End: Sat Apr 13 18:32:42 EDT 2019


Start: Sat Apr 13 18:34:36 EDT 2019
Indexing word vectors.
Found 400000 word vectors.
Found 2469 unique tokens.
Shape of data tensor: (100, 1000)
Shape of label tensor: (100, 96)
Preparing embedding matrix.
Training model.
Train on 80 samples, validate on 20 samples
Epoch 1/1

80/80 [==============================] - 1s 8ms/step - loss: 4.6086 - acc: 0.0125 - val_loss: 4.8921 - val_acc: 0.0000e+00
End: Sat Apr 13 18:35:08 EDT 2019


Start: Sat Apr 13 18:36:15 EDT 2019
Indexing word vectors.
Found 400000 word vectors.
Found 44227 unique tokens.
Shape of data tensor: (10000, 1000)
Shape of label tensor: (10000, 1013)
Preparing embedding matrix.
Training model.
Train on 8000 samples, validate on 2000 samples
Epoch 1/1

 128/8000 [..............................] - ETA: 51s - loss: 6.9226 - acc: 0.0000e+00
 256/8000 [..............................] - ETA: 35s - loss: 6.9561 - acc: 0.0000e+00
 384/8000 [>.............................] - ETA: 30s - loss: 6.9492 - acc: 0.0026
 512/8000 [>.............................] - ETA: 27s - loss: 6.9474 - acc: 0.0020
 640/8000 [=>............................] - ETA: 25s - loss: 6.9444 - acc: 0.0016
 768/8000 [=>............................] - ETA: 24s - loss: 6.9433 - acc: 0.0013
 896/8000 [==>...........................] - ETA: 23s - loss: 6.9397 - acc: 0.0011
1024/8000 [==>...........................] - ETA: 22s - loss: 6.9371 - acc: 9.7656e-04
1152/8000 [===>..........................] - ETA: 21s - loss: 6.9348 - acc: 8.6806e-04
1280/8000 [===>..........................] - ETA: 20s - loss: 6.9338 - acc: 7.8125e-04
1408/8000 [====>.........................] - ETA: 20s - loss: 6.9323 - acc: 7.1023e-04
1536/8000 [====>.........................] - ETA: 19s - loss: 6.9324 - acc: 6.5104e-04
1664/8000 [=====>........................] - ETA: 19s - loss: 6.9313 - acc: 6.0096e-04
1792/8000 [=====>........................] - ETA: 18s - loss: 6.9305 - acc: 0.0011
1920/8000 [======>.......................] - ETA: 18s - loss: 6.9298 - acc: 0.0010
2048/8000 [======>.......................] - ETA: 17s - loss: 6.9297 - acc: 9.7656e-04
2176/8000 [=======>......................] - ETA: 17s - loss: 6.9294 - acc: 9.1912e-04
2304/8000 [=======>......................] - ETA: 16s - loss: 6.9295 - acc: 8.6806e-04
2432/8000 [========>.....................] - ETA: 16s - loss: 6.9290 - acc: 8.2237e-04
2560/8000 [========>.....................] - ETA: 15s - loss: 6.9289 - acc: 7.8125e-04
2688/8000 [=========>....................] - ETA: 15s - loss: 6.9285 - acc: 7.4405e-04
2816/8000 [=========>....................] - ETA: 15s - loss: 6.9278 - acc: 0.0011
2944/8000 [==========>...................] - ETA: 14s - loss: 6.9272 - acc: 0.0010
3072/8000 [==========>...................] - ETA: 14s - loss: 6.9270 - acc: 9.7656e-04
3200/8000 [===========>..................] - ETA: 13s - loss: 6.9266 - acc: 0.0013
3328/8000 [===========>..................] - ETA: 13s - loss: 6.9268 - acc: 0.0012
3456/8000 [===========>..................] - ETA: 13s - loss: 6.9270 - acc: 0.0012
3584/8000 [============>.................] - ETA: 12s - loss: 6.9268 - acc: 0.0011
3712/8000 [============>.................] - ETA: 12s - loss: 6.9267 - acc: 0.0011
3840/8000 [=============>................] - ETA: 11s - loss: 6.9265 - acc: 0.0010
3968/8000 [=============>................] - ETA: 11s - loss: 6.9263 - acc: 0.0010
4096/8000 [==============>...............] - ETA: 11s - loss: 6.9263 - acc: 9.7656e-04
4224/8000 [==============>...............] - ETA: 10s - loss: 6.9260 - acc: 9.4697e-04
4352/8000 [===============>..............] - ETA: 10s - loss: 6.9255 - acc: 0.0011
4480/8000 [===============>..............] - ETA: 10s - loss: 6.9257 - acc: 0.0011
4608/8000 [================>.............] - ETA: 9s - loss: 6.9255 - acc: 0.0013
4736/8000 [================>.............] - ETA: 9s - loss: 6.9255 - acc: 0.0013
4864/8000 [=================>............] - ETA: 9s - loss: 6.9253 - acc: 0.0012
4992/8000 [=================>............] - ETA: 8s - loss: 6.9250 - acc: 0.0012
5120/8000 [==================>...........] - ETA: 8s - loss: 6.9250 - acc: 0.0014
5248/8000 [==================>...........] - ETA: 7s - loss: 6.9249 - acc: 0.0013
5376/8000 [===================>..........] - ETA: 7s - loss: 6.9248 - acc: 0.0013
5504/8000 [===================>..........] - ETA: 7s - loss: 6.9248 - acc: 0.0013
5632/8000 [====================>.........] - ETA: 6s - loss: 6.9247 - acc: 0.0012
5760/8000 [====================>.........] - ETA: 6s - loss: 6.9247 - acc: 0.0012
5888/8000 [=====================>........] - ETA: 6s - loss: 6.9247 - acc: 0.0012
6016/8000 [=====================>........] - ETA: 5s - loss: 6.9246 - acc: 0.0012
6144/8000 [======================>.......] - ETA: 5s - loss: 6.9245 - acc: 0.0011
6272/8000 [======================>.......] - ETA: 4s - loss: 6.9245 - acc: 0.0013
6400/8000 [=======================>......] - ETA: 4s - loss: 6.9244 - acc: 0.0013
6528/8000 [=======================>......] - ETA: 4s - loss: 6.9241 - acc: 0.0014
6656/8000 [=======================>......] - ETA: 3s - loss: 6.9241 - acc: 0.0015
6784/8000 [========================>.....] - ETA: 3s - loss: 6.9243 - acc: 0.0015
6912/8000 [========================>.....] - ETA: 3s - loss: 6.9240 - acc: 0.0014
7040/8000 [=========================>....] - ETA: 2s - loss: 6.9239 - acc: 0.0014
7168/8000 [=========================>....] - ETA: 2s - loss: 6.9239 - acc: 0.0014
7296/8000 [==========================>...] - ETA: 1s - loss: 6.9239 - acc: 0.0015
7424/8000 [==========================>...] - ETA: 1s - loss: 6.9239 - acc: 0.0016
7552/8000 [===========================>..] - ETA: 1s - loss: 6.9240 - acc: 0.0016
7680/8000 [===========================>..] - ETA: 0s - loss: 6.9239 - acc: 0.0017
7808/8000 [============================>.] - ETA: 0s - loss: 6.9238 - acc: 0.0017
7936/8000 [============================>.] - ETA: 0s - loss: 6.9238 - acc: 0.0016
8000/8000 [==============================] - 25s 3ms/step - loss: 6.9239 - acc: 0.0016 - val_loss: 6.9202 - val_acc: 5.0000e-04
End: Sat Apr 13 18:37:17 EDT 2019


Start: Sat Apr 13 18:37:30 EDT 2019
Indexing word vectors.
Found 400000 word vectors.
Found 44227 unique tokens.
Shape of data tensor: (10000, 1000)
Shape of label tensor: (10000, 1013)
Preparing embedding matrix.
Training model.
Train on 8000 samples, validate on 2000 samples
Epoch 1/10

 128/8000 [..............................] - ETA: 53s - loss: 6.9350 - acc: 0.0000e+00
 256/8000 [..............................] - ETA: 36s - loss: 6.9224 - acc: 0.0039
 384/8000 [>.............................] - ETA: 31s - loss: 6.9213 - acc: 0.0026
 512/8000 [>.............................] - ETA: 28s - loss: 6.9306 - acc: 0.0039
 640/8000 [=>............................] - ETA: 26s - loss: 6.9317 - acc: 0.0031
 768/8000 [=>............................] - ETA: 24s - loss: 6.9314 - acc: 0.0026
 896/8000 [==>...........................] - ETA: 23s - loss: 6.9293 - acc: 0.0022
1024/8000 [==>...........................] - ETA: 22s - loss: 6.9283 - acc: 0.0029
1152/8000 [===>..........................] - ETA: 22s - loss: 6.9274 - acc: 0.0026
1280/8000 [===>..........................] - ETA: 21s - loss: 6.9267 - acc: 0.0023
1408/8000 [====>.........................] - ETA: 20s - loss: 6.9252 - acc: 0.0021
1536/8000 [====>.........................] - ETA: 20s - loss: 6.9259 - acc: 0.0020
1664/8000 [=====>........................] - ETA: 19s - loss: 6.9257 - acc: 0.0018
1792/8000 [=====>........................] - ETA: 19s - loss: 6.9247 - acc: 0.0017
1920/8000 [======>.......................] - ETA: 18s - loss: 6.9250 - acc: 0.0016
2048/8000 [======>.......................] - ETA: 18s - loss: 6.9253 - acc: 0.0015
2176/8000 [=======>......................] - ETA: 17s - loss: 6.9253 - acc: 0.0014
2304/8000 [=======>......................] - ETA: 17s - loss: 6.9249 - acc: 0.0017
2432/8000 [========>.....................] - ETA: 17s - loss: 6.9249 - acc: 0.0021
2560/8000 [========>.....................] - ETA: 16s - loss: 6.9248 - acc: 0.0020
2688/8000 [=========>....................] - ETA: 16s - loss: 6.9250 - acc: 0.0019
2816/8000 [=========>....................] - ETA: 15s - loss: 6.9248 - acc: 0.0018
2944/8000 [==========>...................] - ETA: 15s - loss: 6.9248 - acc: 0.0017
3072/8000 [==========>...................] - ETA: 14s - loss: 6.9248 - acc: 0.0020
3200/8000 [===========>..................] - ETA: 14s - loss: 6.9244 - acc: 0.0019
3328/8000 [===========>..................] - ETA: 14s - loss: 6.9241 - acc: 0.0021
3456/8000 [===========>..................] - ETA: 13s - loss: 6.9240 - acc: 0.0020
3584/8000 [============>.................] - ETA: 13s - loss: 6.9237 - acc: 0.0020
3712/8000 [============>.................] - ETA: 12s - loss: 6.9236 - acc: 0.0019
3840/8000 [=============>................] - ETA: 12s - loss: 6.9233 - acc: 0.0018
3968/8000 [=============>................] - ETA: 12s - loss: 6.9235 - acc: 0.0018
4096/8000 [==============>...............] - ETA: 11s - loss: 6.9237 - acc: 0.0017
4224/8000 [==============>...............] - ETA: 11s - loss: 6.9236 - acc: 0.0017
4352/8000 [===============>..............] - ETA: 10s - loss: 6.9236 - acc: 0.0016
4480/8000 [===============>..............] - ETA: 10s - loss: 6.9237 - acc: 0.0016
4608/8000 [================>.............] - ETA: 10s - loss: 6.9239 - acc: 0.0015
4736/8000 [================>.............] - ETA: 9s - loss: 6.9237 - acc: 0.0015
4864/8000 [=================>............] - ETA: 9s - loss: 6.9238 - acc: 0.0014
4992/8000 [=================>............] - ETA: 8s - loss: 6.9237 - acc: 0.0014
5120/8000 [==================>...........] - ETA: 8s - loss: 6.9237 - acc: 0.0014
5248/8000 [==================>...........] - ETA: 8s - loss: 6.9235 - acc: 0.0013
5376/8000 [===================>..........] - ETA: 7s - loss: 6.9234 - acc: 0.0013
5504/8000 [===================>..........] - ETA: 7s - loss: 6.9234 - acc: 0.0015
5632/8000 [====================>.........] - ETA: 7s - loss: 6.9233 - acc: 0.0014
5760/8000 [====================>.........] - ETA: 6s - loss: 6.9233 - acc: 0.0014
5888/8000 [=====================>........] - ETA: 6s - loss: 6.9233 - acc: 0.0014
6016/8000 [=====================>........] - ETA: 5s - loss: 6.9231 - acc: 0.0015
6144/8000 [======================>.......] - ETA: 5s - loss: 6.9230 - acc: 0.0015
6272/8000 [======================>.......] - ETA: 5s - loss: 6.9228 - acc: 0.0014
6400/8000 [=======================>......] - ETA: 4s - loss: 6.9231 - acc: 0.0014
6528/8000 [=======================>......] - ETA: 4s - loss: 6.9230 - acc: 0.0014
6656/8000 [=======================>......] - ETA: 3s - loss: 6.9228 - acc: 0.0014
6784/8000 [========================>.....] - ETA: 3s - loss: 6.9228 - acc: 0.0013
6912/8000 [========================>.....] - ETA: 3s - loss: 6.9229 - acc: 0.0013
7040/8000 [=========================>....] - ETA: 2s - loss: 6.9229 - acc: 0.0013
7168/8000 [=========================>....] - ETA: 2s - loss: 6.9229 - acc: 0.0013
7296/8000 [==========================>...] - ETA: 2s - loss: 6.9228 - acc: 0.0012
7424/8000 [==========================>...] - ETA: 1s - loss: 6.9227 - acc: 0.0012
7552/8000 [===========================>..] - ETA: 1s - loss: 6.9226 - acc: 0.0012
7680/8000 [===========================>..] - ETA: 0s - loss: 6.9226 - acc: 0.0012
7808/8000 [============================>.] - ETA: 0s - loss: 6.9228 - acc: 0.0012
7936/8000 [============================>.] - ETA: 0s - loss: 6.9227 - acc: 0.0011
8000/8000 [==============================] - 25s 3ms/step - loss: 6.9226 - acc: 0.0011 - val_loss: 6.9211 - val_acc: 5.0000e-04
Epoch 2/10

 128/8000 [..............................] - ETA: 23s - loss: 6.9162 - acc: 0.0000e+00
 256/8000 [..............................] - ETA: 22s - loss: 6.9088 - acc: 0.0000e+00
 384/8000 [>.............................] - ETA: 21s - loss: 6.9063 - acc: 0.0000e+00
 512/8000 [>.............................] - ETA: 21s - loss: 6.9069 - acc: 0.0020
 640/8000 [=>............................] - ETA: 21s - loss: 6.9062 - acc: 0.0016
 768/8000 [=>............................] - ETA: 20s - loss: 6.9050 - acc: 0.0013
 896/8000 [==>...........................] - ETA: 20s - loss: 6.9029 - acc: 0.0011
1024/8000 [==>...........................] - ETA: 19s - loss: 6.8977 - acc: 9.7656e-04
1152/8000 [===>..........................] - ETA: 19s - loss: 6.8940 - acc: 0.0026
1280/8000 [===>..........................] - ETA: 19s - loss: 6.8931 - acc: 0.0023
1408/8000 [====>.........................] - ETA: 18s - loss: 6.8936 - acc: 0.0021
1536/8000 [====>.........................] - ETA: 18s - loss: 6.8956 - acc: 0.0020
1664/8000 [=====>........................] - ETA: 17s - loss: 6.8956 - acc: 0.0018
1792/8000 [=====>........................] - ETA: 17s - loss: 6.8970 - acc: 0.0017
1920/8000 [======>.......................] - ETA: 17s - loss: 6.8972 - acc: 0.0016
2048/8000 [======>.......................] - ETA: 16s - loss: 6.8968 - acc: 0.0015
2176/8000 [=======>......................] - ETA: 16s - loss: 6.8966 - acc: 0.0014
2304/8000 [=======>......................] - ETA: 16s - loss: 6.8955 - acc: 0.0013
2432/8000 [========>.....................] - ETA: 15s - loss: 6.8963 - acc: 0.0012
2560/8000 [========>.....................] - ETA: 15s - loss: 6.8964 - acc: 0.0012
2688/8000 [=========>....................] - ETA: 15s - loss: 6.8971 - acc: 0.0015
2816/8000 [=========>....................] - ETA: 14s - loss: 6.8969 - acc: 0.0014
2944/8000 [==========>...................] - ETA: 14s - loss: 6.8962 - acc: 0.0014
3072/8000 [==========>...................] - ETA: 13s - loss: 6.8948 - acc: 0.0013
3200/8000 [===========>..................] - ETA: 13s - loss: 6.8942 - acc: 0.0013
3328/8000 [===========>..................] - ETA: 13s - loss: 6.8945 - acc: 0.0018
3456/8000 [===========>..................] - ETA: 12s - loss: 6.8938 - acc: 0.0017
3584/8000 [============>.................] - ETA: 12s - loss: 6.8931 - acc: 0.0017
3712/8000 [============>.................] - ETA: 12s - loss: 6.8926 - acc: 0.0016
3840/8000 [=============>................] - ETA: 11s - loss: 6.8933 - acc: 0.0016
3968/8000 [=============>................] - ETA: 11s - loss: 6.8937 - acc: 0.0015
4096/8000 [==============>...............] - ETA: 10s - loss: 6.8931 - acc: 0.0015
4224/8000 [==============>...............] - ETA: 10s - loss: 6.8939 - acc: 0.0017
4352/8000 [===============>..............] - ETA: 10s - loss: 6.8939 - acc: 0.0016
4480/8000 [===============>..............] - ETA: 9s - loss: 6.8945 - acc: 0.0016
4608/8000 [================>.............] - ETA: 9s - loss: 6.8943 - acc: 0.0015
4736/8000 [================>.............] - ETA: 9s - loss: 6.8936 - acc: 0.0015
4864/8000 [=================>............] - ETA: 8s - loss: 6.8935 - acc: 0.0014
4992/8000 [=================>............] - ETA: 8s - loss: 6.8938 - acc: 0.0016
5120/8000 [==================>...........] - ETA: 8s - loss: 6.8938 - acc: 0.0018
5248/8000 [==================>...........] - ETA: 7s - loss: 6.8935 - acc: 0.0017
5376/8000 [===================>..........] - ETA: 7s - loss: 6.8929 - acc: 0.0017
5504/8000 [===================>..........] - ETA: 7s - loss: 6.8937 - acc: 0.0016
5632/8000 [====================>.........] - ETA: 6s - loss: 6.8935 - acc: 0.0020
5760/8000 [====================>.........] - ETA: 6s - loss: 6.8938 - acc: 0.0019
5888/8000 [=====================>........] - ETA: 5s - loss: 6.8935 - acc: 0.0019
6016/8000 [=====================>........] - ETA: 5s - loss: 6.8931 - acc: 0.0020
6144/8000 [======================>.......] - ETA: 5s - loss: 6.8926 - acc: 0.0020
6272/8000 [======================>.......] - ETA: 4s - loss: 6.8932 - acc: 0.0019
6400/8000 [=======================>......] - ETA: 4s - loss: 6.8932 - acc: 0.0019
6528/8000 [=======================>......] - ETA: 4s - loss: 6.8935 - acc: 0.0020
6656/8000 [=======================>......] - ETA: 3s - loss: 6.8930 - acc: 0.0020
6784/8000 [========================>.....] - ETA: 3s - loss: 6.8932 - acc: 0.0021
6912/8000 [========================>.....] - ETA: 3s - loss: 6.8926 - acc: 0.0020
7040/8000 [=========================>....] - ETA: 2s - loss: 6.8923 - acc: 0.0021
7168/8000 [=========================>....] - ETA: 2s - loss: 6.8924 - acc: 0.0021
7296/8000 [==========================>...] - ETA: 1s - loss: 6.8919 - acc: 0.0021
7424/8000 [==========================>...] - ETA: 1s - loss: 6.8907 - acc: 0.0020
7552/8000 [===========================>..] - ETA: 1s - loss: 6.8913 - acc: 0.0020
7680/8000 [===========================>..] - ETA: 0s - loss: 6.8907 - acc: 0.0021
7808/8000 [============================>.] - ETA: 0s - loss: 6.8902 - acc: 0.0020
7936/8000 [============================>.] - ETA: 0s - loss: 6.8904 - acc: 0.0021
8000/8000 [==============================] - 25s 3ms/step - loss: 6.8909 - acc: 0.0021 - val_loss: 6.9241 - val_acc: 0.0015
Epoch 3/10

 128/8000 [..............................] - ETA: 22s - loss: 6.8482 - acc: 0.0000e+00
 256/8000 [..............................] - ETA: 22s - loss: 6.8543 - acc: 0.0000e+00
 384/8000 [>.............................] - ETA: 22s - loss: 6.8359 - acc: 0.0000e+00
 512/8000 [>.............................] - ETA: 22s - loss: 6.8320 - acc: 0.0000e+00
 640/8000 [=>............................] - ETA: 21s - loss: 6.8141 - acc: 0.0000e+00
 768/8000 [=>............................] - ETA: 21s - loss: 6.8165 - acc: 0.0000e+00
 896/8000 [==>...........................] - ETA: 20s - loss: 6.8107 - acc: 0.0000e+00
1024/8000 [==>...........................] - ETA: 20s - loss: 6.8039 - acc: 0.0000e+00
1152/8000 [===>..........................] - ETA: 19s - loss: 6.8052 - acc: 8.6806e-04
1280/8000 [===>..........................] - ETA: 19s - loss: 6.8049 - acc: 7.8125e-04
1408/8000 [====>.........................] - ETA: 19s - loss: 6.8013 - acc: 0.0014
1536/8000 [====>.........................] - ETA: 18s - loss: 6.7971 - acc: 0.0020
1664/8000 [=====>........................] - ETA: 18s - loss: 6.7960 - acc: 0.0030
1792/8000 [=====>........................] - ETA: 18s - loss: 6.7910 - acc: 0.0033
1920/8000 [======>.......................] - ETA: 17s - loss: 6.7958 - acc: 0.0036
2048/8000 [======>.......................] - ETA: 17s - loss: 6.7966 - acc: 0.0034
2176/8000 [=======>......................] - ETA: 17s - loss: 6.7906 - acc: 0.0041
2304/8000 [=======>......................] - ETA: 16s - loss: 6.7925 - acc: 0.0039
2432/8000 [========>.....................] - ETA: 16s - loss: 6.7945 - acc: 0.0037
2560/8000 [========>.....................] - ETA: 15s - loss: 6.7940 - acc: 0.0035
2688/8000 [=========>....................] - ETA: 15s - loss: 6.7912 - acc: 0.0037
2816/8000 [=========>....................] - ETA: 15s - loss: 6.7924 - acc: 0.0039
2944/8000 [==========>...................] - ETA: 14s - loss: 6.7931 - acc: 0.0037
3072/8000 [==========>...................] - ETA: 14s - loss: 6.7927 - acc: 0.0046
3200/8000 [===========>..................] - ETA: 13s - loss: 6.7956 - acc: 0.0044
3328/8000 [===========>..................] - ETA: 13s - loss: 6.7955 - acc: 0.0045
3456/8000 [===========>..................] - ETA: 13s - loss: 6.7957 - acc: 0.0043
3584/8000 [============>.................] - ETA: 12s - loss: 6.7986 - acc: 0.0045
3712/8000 [============>.................] - ETA: 12s - loss: 6.7995 - acc: 0.0043
3840/8000 [=============>................] - ETA: 12s - loss: 6.7972 - acc: 0.0047
3968/8000 [=============>................] - ETA: 11s - loss: 6.7927 - acc: 0.0050
4096/8000 [==============>...............] - ETA: 11s - loss: 6.7915 - acc: 0.0049
4224/8000 [==============>...............] - ETA: 10s - loss: 6.7929 - acc: 0.0047
4352/8000 [===============>..............] - ETA: 10s - loss: 6.7924 - acc: 0.0048
4480/8000 [===============>..............] - ETA: 10s - loss: 6.7919 - acc: 0.0047
4608/8000 [================>.............] - ETA: 9s - loss: 6.7920 - acc: 0.0048
4736/8000 [================>.............] - ETA: 9s - loss: 6.7921 - acc: 0.0046
4864/8000 [=================>............] - ETA: 9s - loss: 6.7913 - acc: 0.0047
4992/8000 [=================>............] - ETA: 8s - loss: 6.7925 - acc: 0.0046
5120/8000 [==================>...........] - ETA: 8s - loss: 6.7899 - acc: 0.0047
5248/8000 [==================>...........] - ETA: 8s - loss: 6.7897 - acc: 0.0048
5376/8000 [===================>..........] - ETA: 7s - loss: 6.7893 - acc: 0.0048
5504/8000 [===================>..........] - ETA: 7s - loss: 6.7902 - acc: 0.0047
5632/8000 [====================>.........] - ETA: 6s - loss: 6.7910 - acc: 0.0046
5760/8000 [====================>.........] - ETA: 6s - loss: 6.7926 - acc: 0.0047
5888/8000 [=====================>........] - ETA: 6s - loss: 6.7913 - acc: 0.0046
6016/8000 [=====================>........] - ETA: 5s - loss: 6.7913 - acc: 0.0048
6144/8000 [======================>.......] - ETA: 5s - loss: 6.7910 - acc: 0.0047
6272/8000 [======================>.......] - ETA: 5s - loss: 6.7917 - acc: 0.0046
6400/8000 [=======================>......] - ETA: 4s - loss: 6.7916 - acc: 0.0048
6528/8000 [=======================>......] - ETA: 4s - loss: 6.7907 - acc: 0.0047
6656/8000 [=======================>......] - ETA: 3s - loss: 6.7903 - acc: 0.0047
6784/8000 [========================>.....] - ETA: 3s - loss: 6.7903 - acc: 0.0046
6912/8000 [========================>.....] - ETA: 3s - loss: 6.7908 - acc: 0.0045
7040/8000 [=========================>....] - ETA: 2s - loss: 6.7920 - acc: 0.0044
7168/8000 [=========================>....] - ETA: 2s - loss: 6.7918 - acc: 0.0043
7296/8000 [==========================>...] - ETA: 2s - loss: 6.7916 - acc: 0.0042
7424/8000 [==========================>...] - ETA: 1s - loss: 6.7907 - acc: 0.0044
7552/8000 [===========================>..] - ETA: 1s - loss: 6.7903 - acc: 0.0044
7680/8000 [===========================>..] - ETA: 0s - loss: 6.7896 - acc: 0.0044
7808/8000 [============================>.] - ETA: 0s - loss: 6.7888 - acc: 0.0046
7936/8000 [============================>.] - ETA: 0s - loss: 6.7884 - acc: 0.0045
8000/8000 [==============================] - 25s 3ms/step - loss: 6.7887 - acc: 0.0045 - val_loss: 6.8772 - val_acc: 0.0040
Epoch 4/10

 128/8000 [..............................] - ETA: 22s - loss: 6.5928 - acc: 0.0078
 256/8000 [..............................] - ETA: 21s - loss: 6.6224 - acc: 0.0078
 384/8000 [>.............................] - ETA: 21s - loss: 6.6072 - acc: 0.0078
 512/8000 [>.............................] - ETA: 21s - loss: 6.6154 - acc: 0.0059
 640/8000 [=>............................] - ETA: 20s - loss: 6.6549 - acc: 0.0047
 768/8000 [=>............................] - ETA: 20s - loss: 6.6676 - acc: 0.0065
 896/8000 [==>...........................] - ETA: 20s - loss: 6.6605 - acc: 0.0067
1024/8000 [==>...........................] - ETA: 19s - loss: 6.6528 - acc: 0.0059
1152/8000 [===>..........................] - ETA: 19s - loss: 6.6488 - acc: 0.0052
1280/8000 [===>..........................] - ETA: 19s - loss: 6.6507 - acc: 0.0047
1408/8000 [====>.........................] - ETA: 18s - loss: 6.6574 - acc: 0.0043
1536/8000 [====>.........................] - ETA: 18s - loss: 6.6509 - acc: 0.0046
1664/8000 [=====>........................] - ETA: 17s - loss: 6.6442 - acc: 0.0054
1792/8000 [=====>........................] - ETA: 17s - loss: 6.6449 - acc: 0.0050
1920/8000 [======>.......................] - ETA: 17s - loss: 6.6452 - acc: 0.0047
2048/8000 [======>.......................] - ETA: 17s - loss: 6.6433 - acc: 0.0044
2176/8000 [=======>......................] - ETA: 16s - loss: 6.6373 - acc: 0.0046
2304/8000 [=======>......................] - ETA: 16s - loss: 6.6351 - acc: 0.0048
2432/8000 [========>.....................] - ETA: 15s - loss: 6.6361 - acc: 0.0045
2560/8000 [========>.....................] - ETA: 15s - loss: 6.6388 - acc: 0.0043
2688/8000 [=========>....................] - ETA: 15s - loss: 6.6389 - acc: 0.0045
2816/8000 [=========>....................] - ETA: 14s - loss: 6.6418 - acc: 0.0043
2944/8000 [==========>...................] - ETA: 14s - loss: 6.6424 - acc: 0.0048
3072/8000 [==========>...................] - ETA: 14s - loss: 6.6387 - acc: 0.0049
3200/8000 [===========>..................] - ETA: 13s - loss: 6.6390 - acc: 0.0047
3328/8000 [===========>..................] - ETA: 13s - loss: 6.6408 - acc: 0.0048
3456/8000 [===========>..................] - ETA: 12s - loss: 6.6395 - acc: 0.0049
3584/8000 [============>.................] - ETA: 12s - loss: 6.6381 - acc: 0.0050
3712/8000 [============>.................] - ETA: 12s - loss: 6.6360 - acc: 0.0048
3840/8000 [=============>................] - ETA: 11s - loss: 6.6335 - acc: 0.0049
3968/8000 [=============>................] - ETA: 11s - loss: 6.6351 - acc: 0.0050
4096/8000 [==============>...............] - ETA: 11s - loss: 6.6379 - acc: 0.0049
4224/8000 [==============>...............] - ETA: 10s - loss: 6.6369 - acc: 0.0050
4352/8000 [===============>..............] - ETA: 10s - loss: 6.6375 - acc: 0.0048
4480/8000 [===============>..............] - ETA: 10s - loss: 6.6323 - acc: 0.0047
4608/8000 [================>.............] - ETA: 9s - loss: 6.6304 - acc: 0.0048
4736/8000 [================>.............] - ETA: 9s - loss: 6.6308 - acc: 0.0049
4864/8000 [=================>............] - ETA: 8s - loss: 6.6358 - acc: 0.0047
4992/8000 [=================>............] - ETA: 8s - loss: 6.6352 - acc: 0.0048
5120/8000 [==================>...........] - ETA: 8s - loss: 6.6345 - acc: 0.0049
5248/8000 [==================>...........] - ETA: 7s - loss: 6.6338 - acc: 0.0050
5376/8000 [===================>..........] - ETA: 7s - loss: 6.6328 - acc: 0.0048
5504/8000 [===================>..........] - ETA: 7s - loss: 6.6336 - acc: 0.0047
5632/8000 [====================>.........] - ETA: 6s - loss: 6.6333 - acc: 0.0046
5760/8000 [====================>.........] - ETA: 6s - loss: 6.6334 - acc: 0.0047
5888/8000 [=====================>........] - ETA: 6s - loss: 6.6348 - acc: 0.0046
6016/8000 [=====================>........] - ETA: 5s - loss: 6.6358 - acc: 0.0045
6144/8000 [======================>.......] - ETA: 5s - loss: 6.6355 - acc: 0.0044
6272/8000 [======================>.......] - ETA: 4s - loss: 6.6342 - acc: 0.0045
6400/8000 [=======================>......] - ETA: 4s - loss: 6.6325 - acc: 0.0044
6528/8000 [=======================>......] - ETA: 4s - loss: 6.6325 - acc: 0.0043
6656/8000 [=======================>......] - ETA: 3s - loss: 6.6343 - acc: 0.0042
6784/8000 [========================>.....] - ETA: 3s - loss: 6.6353 - acc: 0.0044
6912/8000 [========================>.....] - ETA: 3s - loss: 6.6333 - acc: 0.0046
7040/8000 [=========================>....] - ETA: 2s - loss: 6.6334 - acc: 0.0047
7168/8000 [=========================>....] - ETA: 2s - loss: 6.6313 - acc: 0.0047
7296/8000 [==========================>...] - ETA: 2s - loss: 6.6297 - acc: 0.0047
7424/8000 [==========================>...] - ETA: 1s - loss: 6.6292 - acc: 0.0047
7552/8000 [===========================>..] - ETA: 1s - loss: 6.6287 - acc: 0.0048
7680/8000 [===========================>..] - ETA: 0s - loss: 6.6278 - acc: 0.0047
7808/8000 [============================>.] - ETA: 0s - loss: 6.6283 - acc: 0.0046
7936/8000 [============================>.] - ETA: 0s - loss: 6.6282 - acc: 0.0050
8000/8000 [==============================] - 25s 3ms/step - loss: 6.6265 - acc: 0.0050 - val_loss: 6.8217 - val_acc: 0.0020
Epoch 5/10

 128/8000 [..............................] - ETA: 21s - loss: 6.4318 - acc: 0.0234
 256/8000 [..............................] - ETA: 21s - loss: 6.4087 - acc: 0.0195
 384/8000 [>.............................] - ETA: 21s - loss: 6.4109 - acc: 0.0156
 512/8000 [>.............................] - ETA: 22s - loss: 6.4469 - acc: 0.0117
 640/8000 [=>............................] - ETA: 21s - loss: 6.4514 - acc: 0.0109
 768/8000 [=>............................] - ETA: 21s - loss: 6.4484 - acc: 0.0104
 896/8000 [==>...........................] - ETA: 20s - loss: 6.4403 - acc: 0.0089
1024/8000 [==>...........................] - ETA: 20s - loss: 6.4359 - acc: 0.0107
1152/8000 [===>..........................] - ETA: 19s - loss: 6.4285 - acc: 0.0095
1280/8000 [===>..........................] - ETA: 19s - loss: 6.4236 - acc: 0.0094
1408/8000 [====>.........................] - ETA: 19s - loss: 6.4302 - acc: 0.0099
1536/8000 [====>.........................] - ETA: 18s - loss: 6.4352 - acc: 0.0091
1664/8000 [=====>........................] - ETA: 18s - loss: 6.4371 - acc: 0.0090
1792/8000 [=====>........................] - ETA: 17s - loss: 6.4331 - acc: 0.0084
1920/8000 [======>.......................] - ETA: 17s - loss: 6.4282 - acc: 0.0083
2048/8000 [======>.......................] - ETA: 17s - loss: 6.4367 - acc: 0.0083
2176/8000 [=======>......................] - ETA: 16s - loss: 6.4422 - acc: 0.0078
2304/8000 [=======>......................] - ETA: 16s - loss: 6.4416 - acc: 0.0082
2432/8000 [========>.....................] - ETA: 16s - loss: 6.4407 - acc: 0.0086
2560/8000 [========>.....................] - ETA: 15s - loss: 6.4384 - acc: 0.0086
2688/8000 [=========>....................] - ETA: 15s - loss: 6.4392 - acc: 0.0086
2816/8000 [=========>....................] - ETA: 15s - loss: 6.4370 - acc: 0.0092
2944/8000 [==========>...................] - ETA: 14s - loss: 6.4361 - acc: 0.0095
3072/8000 [==========>...................] - ETA: 14s - loss: 6.4373 - acc: 0.0094
3200/8000 [===========>..................] - ETA: 14s - loss: 6.4420 - acc: 0.0094
3328/8000 [===========>..................] - ETA: 13s - loss: 6.4370 - acc: 0.0102
3456/8000 [===========>..................] - ETA: 13s - loss: 6.4387 - acc: 0.0098
3584/8000 [============>.................] - ETA: 12s - loss: 6.4405 - acc: 0.0095
3712/8000 [============>.................] - ETA: 12s - loss: 6.4417 - acc: 0.0097
3840/8000 [=============>................] - ETA: 12s - loss: 6.4424 - acc: 0.0102
3968/8000 [=============>................] - ETA: 11s - loss: 6.4408 - acc: 0.0108
4096/8000 [==============>...............] - ETA: 11s - loss: 6.4393 - acc: 0.0105
4224/8000 [==============>...............] - ETA: 11s - loss: 6.4368 - acc: 0.0104
4352/8000 [===============>..............] - ETA: 10s - loss: 6.4314 - acc: 0.0106
4480/8000 [===============>..............] - ETA: 10s - loss: 6.4320 - acc: 0.0103
4608/8000 [================>.............] - ETA: 9s - loss: 6.4309 - acc: 0.0102
4736/8000 [================>.............] - ETA: 9s - loss: 6.4290 - acc: 0.0106
4864/8000 [=================>............] - ETA: 9s - loss: 6.4302 - acc: 0.0105
4992/8000 [=================>............] - ETA: 8s - loss: 6.4295 - acc: 0.0106
5120/8000 [==================>...........] - ETA: 8s - loss: 6.4353 - acc: 0.0105
5248/8000 [==================>...........] - ETA: 8s - loss: 6.4363 - acc: 0.0105
5376/8000 [===================>..........] - ETA: 7s - loss: 6.4373 - acc: 0.0104
5504/8000 [===================>..........] - ETA: 7s - loss: 6.4389 - acc: 0.0105
5632/8000 [====================>.........] - ETA: 6s - loss: 6.4377 - acc: 0.0105
5760/8000 [====================>.........] - ETA: 6s - loss: 6.4386 - acc: 0.0102
5888/8000 [=====================>........] - ETA: 6s - loss: 6.4400 - acc: 0.0104
6016/8000 [=====================>........] - ETA: 5s - loss: 6.4408 - acc: 0.0101
6144/8000 [======================>.......] - ETA: 5s - loss: 6.4394 - acc: 0.0104
6272/8000 [======================>.......] - ETA: 5s - loss: 6.4369 - acc: 0.0110
6400/8000 [=======================>......] - ETA: 4s - loss: 6.4353 - acc: 0.0108
6528/8000 [=======================>......] - ETA: 4s - loss: 6.4335 - acc: 0.0107
6656/8000 [=======================>......] - ETA: 3s - loss: 6.4368 - acc: 0.0108
6784/8000 [========================>.....] - ETA: 3s - loss: 6.4378 - acc: 0.0106
6912/8000 [========================>.....] - ETA: 3s - loss: 6.4375 - acc: 0.0106
7040/8000 [=========================>....] - ETA: 2s - loss: 6.4382 - acc: 0.0107
7168/8000 [=========================>....] - ETA: 2s - loss: 6.4388 - acc: 0.0105
7296/8000 [==========================>...] - ETA: 2s - loss: 6.4393 - acc: 0.0103
7424/8000 [==========================>...] - ETA: 1s - loss: 6.4404 - acc: 0.0102
7552/8000 [===========================>..] - ETA: 1s - loss: 6.4424 - acc: 0.0102
7680/8000 [===========================>..] - ETA: 0s - loss: 6.4418 - acc: 0.0102
7808/8000 [============================>.] - ETA: 0s - loss: 6.4392 - acc: 0.0101
7936/8000 [============================>.] - ETA: 0s - loss: 6.4385 - acc: 0.0103
8000/8000 [==============================] - 25s 3ms/step - loss: 6.4390 - acc: 0.0103 - val_loss: 6.7675 - val_acc: 0.0050
Epoch 6/10

 128/8000 [..............................] - ETA: 23s - loss: 6.2978 - acc: 0.0078
 256/8000 [..............................] - ETA: 22s - loss: 6.1870 - acc: 0.0078
 384/8000 [>.............................] - ETA: 22s - loss: 6.2022 - acc: 0.0156
 512/8000 [>.............................] - ETA: 22s - loss: 6.2044 - acc: 0.0176
 640/8000 [=>............................] - ETA: 21s - loss: 6.2704 - acc: 0.0156
 768/8000 [=>............................] - ETA: 21s - loss: 6.2616 - acc: 0.0182
 896/8000 [==>...........................] - ETA: 20s - loss: 6.2830 - acc: 0.0167
1024/8000 [==>...........................] - ETA: 20s - loss: 6.2816 - acc: 0.0156
1152/8000 [===>..........................] - ETA: 20s - loss: 6.2634 - acc: 0.0156
1280/8000 [===>..........................] - ETA: 19s - loss: 6.2344 - acc: 0.0148
1408/8000 [====>.........................] - ETA: 19s - loss: 6.2324 - acc: 0.0149
1536/8000 [====>.........................] - ETA: 18s - loss: 6.2361 - acc: 0.0150
1664/8000 [=====>........................] - ETA: 18s - loss: 6.2369 - acc: 0.0144
1792/8000 [=====>........................] - ETA: 18s - loss: 6.2444 - acc: 0.0145
1920/8000 [======>.......................] - ETA: 17s - loss: 6.2599 - acc: 0.0135
2048/8000 [======>.......................] - ETA: 17s - loss: 6.2536 - acc: 0.0142
2176/8000 [=======>......................] - ETA: 17s - loss: 6.2554 - acc: 0.0138
2304/8000 [=======>......................] - ETA: 16s - loss: 6.2510 - acc: 0.0135
2432/8000 [========>.....................] - ETA: 16s - loss: 6.2466 - acc: 0.0132
2560/8000 [========>.....................] - ETA: 16s - loss: 6.2397 - acc: 0.0129
2688/8000 [=========>....................] - ETA: 15s - loss: 6.2361 - acc: 0.0141
2816/8000 [=========>....................] - ETA: 15s - loss: 6.2323 - acc: 0.0142
2944/8000 [==========>...................] - ETA: 14s - loss: 6.2318 - acc: 0.0139
3072/8000 [==========>...................] - ETA: 14s - loss: 6.2293 - acc: 0.0140
3200/8000 [===========>..................] - ETA: 14s - loss: 6.2327 - acc: 0.0144
3328/8000 [===========>..................] - ETA: 13s - loss: 6.2291 - acc: 0.0141
3456/8000 [===========>..................] - ETA: 13s - loss: 6.2291 - acc: 0.0142
3584/8000 [============>.................] - ETA: 12s - loss: 6.2334 - acc: 0.0140
3712/8000 [============>.................] - ETA: 12s - loss: 6.2367 - acc: 0.0140
3840/8000 [=============>................] - ETA: 12s - loss: 6.2338 - acc: 0.0141
3968/8000 [=============>................] - ETA: 11s - loss: 6.2343 - acc: 0.0136
4096/8000 [==============>...............] - ETA: 11s - loss: 6.2325 - acc: 0.0139
4224/8000 [==============>...............] - ETA: 10s - loss: 6.2288 - acc: 0.0142
4352/8000 [===============>..............] - ETA: 10s - loss: 6.2206 - acc: 0.0138
4480/8000 [===============>..............] - ETA: 10s - loss: 6.2188 - acc: 0.0134
4608/8000 [================>.............] - ETA: 9s - loss: 6.2180 - acc: 0.0137
4736/8000 [================>.............] - ETA: 9s - loss: 6.2148 - acc: 0.0135
4864/8000 [=================>............] - ETA: 9s - loss: 6.2137 - acc: 0.0132
4992/8000 [=================>............] - ETA: 8s - loss: 6.2139 - acc: 0.0128
5120/8000 [==================>...........] - ETA: 8s - loss: 6.2152 - acc: 0.0127
5248/8000 [==================>...........] - ETA: 7s - loss: 6.2150 - acc: 0.0124
5376/8000 [===================>..........] - ETA: 7s - loss: 6.2119 - acc: 0.0125
5504/8000 [===================>..........] - ETA: 7s - loss: 6.2155 - acc: 0.0125
5632/8000 [====================>.........] - ETA: 6s - loss: 6.2196 - acc: 0.0123
5760/8000 [====================>.........] - ETA: 6s - loss: 6.2209 - acc: 0.0120
5888/8000 [=====================>........] - ETA: 6s - loss: 6.2216 - acc: 0.0121
6016/8000 [=====================>........] - ETA: 5s - loss: 6.2237 - acc: 0.0120
6144/8000 [======================>.......] - ETA: 5s - loss: 6.2241 - acc: 0.0122
6272/8000 [======================>.......] - ETA: 5s - loss: 6.2231 - acc: 0.0123
6400/8000 [=======================>......] - ETA: 4s - loss: 6.2235 - acc: 0.0125
6528/8000 [=======================>......] - ETA: 4s - loss: 6.2256 - acc: 0.0124
6656/8000 [=======================>......] - ETA: 3s - loss: 6.2217 - acc: 0.0126
6784/8000 [========================>.....] - ETA: 3s - loss: 6.2229 - acc: 0.0125
6912/8000 [========================>.....] - ETA: 3s - loss: 6.2259 - acc: 0.0123
7040/8000 [=========================>....] - ETA: 2s - loss: 6.2292 - acc: 0.0122
7168/8000 [=========================>....] - ETA: 2s - loss: 6.2290 - acc: 0.0121
7296/8000 [==========================>...] - ETA: 2s - loss: 6.2296 - acc: 0.0122
7424/8000 [==========================>...] - ETA: 1s - loss: 6.2269 - acc: 0.0123
7552/8000 [===========================>..] - ETA: 1s - loss: 6.2287 - acc: 0.0122
7680/8000 [===========================>..] - ETA: 0s - loss: 6.2268 - acc: 0.0121
7808/8000 [============================>.] - ETA: 0s - loss: 6.2257 - acc: 0.0123
7936/8000 [============================>.] - ETA: 0s - loss: 6.2280 - acc: 0.0122
8000/8000 [==============================] - 25s 3ms/step - loss: 6.2296 - acc: 0.0123 - val_loss: 6.6942 - val_acc: 0.0070
Epoch 7/10

 128/8000 [..............................] - ETA: 22s - loss: 5.9944 - acc: 0.0156
 256/8000 [..............................] - ETA: 22s - loss: 5.9135 - acc: 0.0430
 384/8000 [>.............................] - ETA: 23s - loss: 5.9343 - acc: 0.0339
 512/8000 [>.............................] - ETA: 22s - loss: 5.9124 - acc: 0.0371
 640/8000 [=>............................] - ETA: 21s - loss: 5.9794 - acc: 0.0344
 768/8000 [=>............................] - ETA: 21s - loss: 6.0452 - acc: 0.0299
 896/8000 [==>...........................] - ETA: 20s - loss: 6.0402 - acc: 0.0279
1024/8000 [==>...........................] - ETA: 20s - loss: 6.0191 - acc: 0.0303
1152/8000 [===>..........................] - ETA: 19s - loss: 6.0186 - acc: 0.0286
1280/8000 [===>..........................] - ETA: 19s - loss: 5.9936 - acc: 0.0305
1408/8000 [====>.........................] - ETA: 19s - loss: 5.9654 - acc: 0.0284
1536/8000 [====>.........................] - ETA: 18s - loss: 5.9573 - acc: 0.0280
1664/8000 [=====>........................] - ETA: 18s - loss: 5.9817 - acc: 0.0276
1792/8000 [=====>........................] - ETA: 18s - loss: 5.9736 - acc: 0.0273
1920/8000 [======>.......................] - ETA: 17s - loss: 5.9680 - acc: 0.0266
2048/8000 [======>.......................] - ETA: 17s - loss: 5.9591 - acc: 0.0259
2176/8000 [=======>......................] - ETA: 16s - loss: 5.9640 - acc: 0.0244
2304/8000 [=======>......................] - ETA: 16s - loss: 5.9758 - acc: 0.0243
2432/8000 [========>.....................] - ETA: 16s - loss: 5.9749 - acc: 0.0238
2560/8000 [========>.....................] - ETA: 15s - loss: 5.9744 - acc: 0.0230
2688/8000 [=========>....................] - ETA: 15s - loss: 5.9819 - acc: 0.0238
2816/8000 [=========>....................] - ETA: 15s - loss: 5.9819 - acc: 0.0231
2944/8000 [==========>...................] - ETA: 14s - loss: 5.9723 - acc: 0.0231
3072/8000 [==========>...................] - ETA: 14s - loss: 5.9729 - acc: 0.0241
3200/8000 [===========>..................] - ETA: 13s - loss: 5.9731 - acc: 0.0234
3328/8000 [===========>..................] - ETA: 13s - loss: 5.9745 - acc: 0.0234
3456/8000 [===========>..................] - ETA: 13s - loss: 5.9812 - acc: 0.0231
3584/8000 [============>.................] - ETA: 12s - loss: 5.9882 - acc: 0.0229
3712/8000 [============>.................] - ETA: 12s - loss: 5.9859 - acc: 0.0229
3840/8000 [=============>................] - ETA: 12s - loss: 5.9803 - acc: 0.0227
3968/8000 [=============>................] - ETA: 11s - loss: 5.9765 - acc: 0.0229
4096/8000 [==============>...............] - ETA: 11s - loss: 5.9764 - acc: 0.0229
4224/8000 [==============>...............] - ETA: 10s - loss: 5.9854 - acc: 0.0230
4352/8000 [===============>..............] - ETA: 10s - loss: 5.9927 - acc: 0.0227
4480/8000 [===============>..............] - ETA: 10s - loss: 5.9921 - acc: 0.0225
4608/8000 [================>.............] - ETA: 9s - loss: 5.9921 - acc: 0.0224
4736/8000 [================>.............] - ETA: 9s - loss: 5.9888 - acc: 0.0220
4864/8000 [=================>............] - ETA: 9s - loss: 5.9920 - acc: 0.0214
4992/8000 [=================>............] - ETA: 8s - loss: 5.9949 - acc: 0.0210
5120/8000 [==================>...........] - ETA: 8s - loss: 6.0013 - acc: 0.0209
5248/8000 [==================>...........] - ETA: 7s - loss: 5.9985 - acc: 0.0212
5376/8000 [===================>..........] - ETA: 7s - loss: 5.9973 - acc: 0.0212
5504/8000 [===================>..........] - ETA: 7s - loss: 5.9966 - acc: 0.0211
5632/8000 [====================>.........] - ETA: 6s - loss: 5.9948 - acc: 0.0218
5760/8000 [====================>.........] - ETA: 6s - loss: 5.9971 - acc: 0.0219
5888/8000 [=====================>........] - ETA: 6s - loss: 5.9931 - acc: 0.0219
6016/8000 [=====================>........] - ETA: 5s - loss: 5.9896 - acc: 0.0223
6144/8000 [======================>.......] - ETA: 5s - loss: 5.9931 - acc: 0.0220
6272/8000 [======================>.......] - ETA: 4s - loss: 5.9961 - acc: 0.0220
6400/8000 [=======================>......] - ETA: 4s - loss: 5.9945 - acc: 0.0217
6528/8000 [=======================>......] - ETA: 4s - loss: 5.9940 - acc: 0.0218
6656/8000 [=======================>......] - ETA: 3s - loss: 5.9914 - acc: 0.0221
6784/8000 [========================>.....] - ETA: 3s - loss: 5.9930 - acc: 0.0221
6912/8000 [========================>.....] - ETA: 3s - loss: 5.9981 - acc: 0.0218
7040/8000 [=========================>....] - ETA: 2s - loss: 5.9943 - acc: 0.0224
7168/8000 [=========================>....] - ETA: 2s - loss: 5.9916 - acc: 0.0226
7296/8000 [==========================>...] - ETA: 2s - loss: 5.9900 - acc: 0.0222
7424/8000 [==========================>...] - ETA: 1s - loss: 5.9936 - acc: 0.0218
7552/8000 [===========================>..] - ETA: 1s - loss: 5.9953 - acc: 0.0217
7680/8000 [===========================>..] - ETA: 0s - loss: 5.9923 - acc: 0.0217
7808/8000 [============================>.] - ETA: 0s - loss: 5.9930 - acc: 0.0215
7936/8000 [============================>.] - ETA: 0s - loss: 5.9990 - acc: 0.0213
8000/8000 [==============================] - 25s 3ms/step - loss: 6.0004 - acc: 0.0211 - val_loss: 6.6327 - val_acc: 0.0040
Epoch 8/10

 128/8000 [..............................] - ETA: 22s - loss: 5.7420 - acc: 0.0547
 256/8000 [..............................] - ETA: 22s - loss: 5.6862 - acc: 0.0469
 384/8000 [>.............................] - ETA: 21s - loss: 5.7250 - acc: 0.0339
 512/8000 [>.............................] - ETA: 21s - loss: 5.7124 - acc: 0.0352
 640/8000 [=>............................] - ETA: 21s - loss: 5.7474 - acc: 0.0328
 768/8000 [=>............................] - ETA: 20s - loss: 5.7476 - acc: 0.0339
 896/8000 [==>...........................] - ETA: 20s - loss: 5.7398 - acc: 0.0346
1024/8000 [==>...........................] - ETA: 20s - loss: 5.7250 - acc: 0.0361
1152/8000 [===>..........................] - ETA: 19s - loss: 5.7175 - acc: 0.0382
1280/8000 [===>..........................] - ETA: 19s - loss: 5.7105 - acc: 0.0367
1408/8000 [====>.........................] - ETA: 18s - loss: 5.7263 - acc: 0.0369
1536/8000 [====>.........................] - ETA: 18s - loss: 5.7134 - acc: 0.0371
1664/8000 [=====>........................] - ETA: 18s - loss: 5.7019 - acc: 0.0349
1792/8000 [=====>........................] - ETA: 17s - loss: 5.6998 - acc: 0.0346
1920/8000 [======>.......................] - ETA: 17s - loss: 5.6923 - acc: 0.0344
2048/8000 [======>.......................] - ETA: 17s - loss: 5.6888 - acc: 0.0337
2176/8000 [=======>......................] - ETA: 16s - loss: 5.6943 - acc: 0.0331
2304/8000 [=======>......................] - ETA: 16s - loss: 5.6994 - acc: 0.0317
2432/8000 [========>.....................] - ETA: 16s - loss: 5.6933 - acc: 0.0308
2560/8000 [========>.....................] - ETA: 15s - loss: 5.6959 - acc: 0.0301
2688/8000 [=========>....................] - ETA: 15s - loss: 5.7128 - acc: 0.0290
2816/8000 [=========>....................] - ETA: 14s - loss: 5.7149 - acc: 0.0284
2944/8000 [==========>...................] - ETA: 14s - loss: 5.7112 - acc: 0.0282
3072/8000 [==========>...................] - ETA: 14s - loss: 5.7031 - acc: 0.0280
3200/8000 [===========>..................] - ETA: 13s - loss: 5.6994 - acc: 0.0288
3328/8000 [===========>..................] - ETA: 13s - loss: 5.6984 - acc: 0.0288
3456/8000 [===========>..................] - ETA: 13s - loss: 5.7102 - acc: 0.0284
3584/8000 [============>.................] - ETA: 12s - loss: 5.7188 - acc: 0.0282
3712/8000 [============>.................] - ETA: 12s - loss: 5.7237 - acc: 0.0286
3840/8000 [=============>................] - ETA: 12s - loss: 5.7316 - acc: 0.0281
3968/8000 [=============>................] - ETA: 11s - loss: 5.7303 - acc: 0.0277
4096/8000 [==============>...............] - ETA: 11s - loss: 5.7317 - acc: 0.0278
4224/8000 [==============>...............] - ETA: 10s - loss: 5.7290 - acc: 0.0275
4352/8000 [===============>..............] - ETA: 10s - loss: 5.7281 - acc: 0.0280
4480/8000 [===============>..............] - ETA: 10s - loss: 5.7308 - acc: 0.0279
4608/8000 [================>.............] - ETA: 9s - loss: 5.7319 - acc: 0.0276
4736/8000 [================>.............] - ETA: 9s - loss: 5.7319 - acc: 0.0270
4864/8000 [=================>............] - ETA: 9s - loss: 5.7282 - acc: 0.0267
4992/8000 [=================>............] - ETA: 8s - loss: 5.7313 - acc: 0.0262
5120/8000 [==================>...........] - ETA: 8s - loss: 5.7307 - acc: 0.0264
5248/8000 [==================>...........] - ETA: 8s - loss: 5.7271 - acc: 0.0267
5376/8000 [===================>..........] - ETA: 7s - loss: 5.7263 - acc: 0.0266
5504/8000 [===================>..........] - ETA: 7s - loss: 5.7233 - acc: 0.0273
5632/8000 [====================>.........] - ETA: 6s - loss: 5.7209 - acc: 0.0275
5760/8000 [====================>.........] - ETA: 6s - loss: 5.7228 - acc: 0.0271
5888/8000 [=====================>........] - ETA: 6s - loss: 5.7325 - acc: 0.0268
6016/8000 [=====================>........] - ETA: 5s - loss: 5.7345 - acc: 0.0268
6144/8000 [======================>.......] - ETA: 5s - loss: 5.7370 - acc: 0.0265
6272/8000 [======================>.......] - ETA: 5s - loss: 5.7348 - acc: 0.0265
6400/8000 [=======================>......] - ETA: 4s - loss: 5.7362 - acc: 0.0261
6528/8000 [=======================>......] - ETA: 4s - loss: 5.7322 - acc: 0.0265
6656/8000 [=======================>......] - ETA: 3s - loss: 5.7299 - acc: 0.0267
6784/8000 [========================>.....] - ETA: 3s - loss: 5.7326 - acc: 0.0273
6912/8000 [========================>.....] - ETA: 3s - loss: 5.7360 - acc: 0.0272
7040/8000 [=========================>....] - ETA: 2s - loss: 5.7388 - acc: 0.0270
7168/8000 [=========================>....] - ETA: 2s - loss: 5.7397 - acc: 0.0271
7296/8000 [==========================>...] - ETA: 2s - loss: 5.7433 - acc: 0.0273
7424/8000 [==========================>...] - ETA: 1s - loss: 5.7421 - acc: 0.0275
7552/8000 [===========================>..] - ETA: 1s - loss: 5.7417 - acc: 0.0273
7680/8000 [===========================>..] - ETA: 0s - loss: 5.7406 - acc: 0.0276
7808/8000 [============================>.] - ETA: 0s - loss: 5.7417 - acc: 0.0273
7936/8000 [============================>.] - ETA: 0s - loss: 5.7397 - acc: 0.0273
8000/8000 [==============================] - 27s 3ms/step - loss: 5.7398 - acc: 0.0274 - val_loss: 6.8561 - val_acc: 0.0080
Epoch 9/10

 128/8000 [..............................] - ETA: 1:00 - loss: 5.4142 - acc: 0.0312
 256/8000 [..............................] - ETA: 59s - loss: 5.5943 - acc: 0.0312
 384/8000 [>.............................] - ETA: 1:01 - loss: 5.7167 - acc: 0.0286
 512/8000 [>.............................] - ETA: 58s - loss: 5.6180 - acc: 0.0312
 640/8000 [=>............................] - ETA: 58s - loss: 5.5770 - acc: 0.0344
 768/8000 [=>............................] - ETA: 56s - loss: 5.5360 - acc: 0.0378
 896/8000 [==>...........................] - ETA: 56s - loss: 5.5359 - acc: 0.0368
1024/8000 [==>...........................] - ETA: 55s - loss: 5.5350 - acc: 0.0381
1152/8000 [===>..........................] - ETA: 54s - loss: 5.5251 - acc: 0.0382
1280/8000 [===>..........................] - ETA: 52s - loss: 5.5191 - acc: 0.0375
1408/8000 [====>.........................] - ETA: 51s - loss: 5.5106 - acc: 0.0405
1536/8000 [====>.........................] - ETA: 50s - loss: 5.4993 - acc: 0.0404
1664/8000 [=====>........................] - ETA: 49s - loss: 5.5045 - acc: 0.0415
1792/8000 [=====>........................] - ETA: 48s - loss: 5.5075 - acc: 0.0402
1920/8000 [======>.......................] - ETA: 45s - loss: 5.5037 - acc: 0.0401
2048/8000 [======>.......................] - ETA: 42s - loss: 5.4994 - acc: 0.0386
2176/8000 [=======>......................] - ETA: 40s - loss: 5.4950 - acc: 0.0386
2304/8000 [=======>......................] - ETA: 38s - loss: 5.4882 - acc: 0.0399
2432/8000 [========>.....................] - ETA: 36s - loss: 5.4771 - acc: 0.0407
2560/8000 [========>.....................] - ETA: 34s - loss: 5.4694 - acc: 0.0402
2688/8000 [=========>....................] - ETA: 32s - loss: 5.4739 - acc: 0.0398
2816/8000 [=========>....................] - ETA: 31s - loss: 5.4775 - acc: 0.0398
2944/8000 [==========>...................] - ETA: 29s - loss: 5.4739 - acc: 0.0401
3072/8000 [==========>...................] - ETA: 28s - loss: 5.4665 - acc: 0.0413
3200/8000 [===========>..................] - ETA: 27s - loss: 5.4723 - acc: 0.0409
3328/8000 [===========>..................] - ETA: 25s - loss: 5.4879 - acc: 0.0403
3456/8000 [===========>..................] - ETA: 24s - loss: 5.4920 - acc: 0.0402
3584/8000 [============>.................] - ETA: 23s - loss: 5.4902 - acc: 0.0407
3712/8000 [============>.................] - ETA: 22s - loss: 5.4789 - acc: 0.0412
3840/8000 [=============>................] - ETA: 21s - loss: 5.4732 - acc: 0.0427
3968/8000 [=============>................] - ETA: 20s - loss: 5.4703 - acc: 0.0433
4096/8000 [==============>...............] - ETA: 19s - loss: 5.4721 - acc: 0.0435
4224/8000 [==============>...............] - ETA: 18s - loss: 5.4873 - acc: 0.0426
4352/8000 [===============>..............] - ETA: 18s - loss: 5.4939 - acc: 0.0418
4480/8000 [===============>..............] - ETA: 17s - loss: 5.4943 - acc: 0.0415
4608/8000 [================>.............] - ETA: 16s - loss: 5.4981 - acc: 0.0408
4736/8000 [================>.............] - ETA: 15s - loss: 5.5029 - acc: 0.0408
4864/8000 [=================>............] - ETA: 14s - loss: 5.5024 - acc: 0.0405
4992/8000 [=================>............] - ETA: 14s - loss: 5.4979 - acc: 0.0405
5120/8000 [==================>...........] - ETA: 13s - loss: 5.4954 - acc: 0.0400
5248/8000 [==================>...........] - ETA: 12s - loss: 5.4915 - acc: 0.0400
5376/8000 [===================>..........] - ETA: 11s - loss: 5.4888 - acc: 0.0400
5504/8000 [===================>..........] - ETA: 11s - loss: 5.4873 - acc: 0.0398
5632/8000 [====================>.........] - ETA: 10s - loss: 5.4892 - acc: 0.0401
5760/8000 [====================>.........] - ETA: 9s - loss: 5.4865 - acc: 0.0410
5888/8000 [=====================>........] - ETA: 9s - loss: 5.4876 - acc: 0.0409
6016/8000 [=====================>........] - ETA: 8s - loss: 5.4884 - acc: 0.0411
6144/8000 [======================>.......] - ETA: 8s - loss: 5.4905 - acc: 0.0402
6272/8000 [======================>.......] - ETA: 7s - loss: 5.4939 - acc: 0.0399
6400/8000 [=======================>......] - ETA: 6s - loss: 5.4935 - acc: 0.0397
6528/8000 [=======================>......] - ETA: 6s - loss: 5.4926 - acc: 0.0392
6656/8000 [=======================>......] - ETA: 5s - loss: 5.4975 - acc: 0.0388
6784/8000 [========================>.....] - ETA: 5s - loss: 5.4998 - acc: 0.0386
6912/8000 [========================>.....] - ETA: 4s - loss: 5.5041 - acc: 0.0385
7040/8000 [=========================>....] - ETA: 3s - loss: 5.5032 - acc: 0.0382
7168/8000 [=========================>....] - ETA: 3s - loss: 5.5021 - acc: 0.0388
7296/8000 [==========================>...] - ETA: 2s - loss: 5.5006 - acc: 0.0388
7424/8000 [==========================>...] - ETA: 2s - loss: 5.4995 - acc: 0.0392
7552/8000 [===========================>..] - ETA: 1s - loss: 5.5043 - acc: 0.0388
7680/8000 [===========================>..] - ETA: 1s - loss: 5.5036 - acc: 0.0392
7808/8000 [============================>.] - ETA: 0s - loss: 5.5053 - acc: 0.0388
7936/8000 [============================>.] - ETA: 0s - loss: 5.5064 - acc: 0.0386
8000/8000 [==============================] - 34s 4ms/step - loss: 5.5084 - acc: 0.0385 - val_loss: 6.8404 - val_acc: 0.0070
Epoch 10/10

 128/8000 [..............................] - ETA: 22s - loss: 5.3137 - acc: 0.0703
 256/8000 [..............................] - ETA: 22s - loss: 5.1763 - acc: 0.0781
 384/8000 [>.............................] - ETA: 21s - loss: 5.1656 - acc: 0.0833
 512/8000 [>.............................] - ETA: 21s - loss: 5.2406 - acc: 0.0703
 640/8000 [=>............................] - ETA: 21s - loss: 5.1915 - acc: 0.0703
 768/8000 [=>............................] - ETA: 20s - loss: 5.1933 - acc: 0.0677
 896/8000 [==>...........................] - ETA: 20s - loss: 5.1958 - acc: 0.0647
1024/8000 [==>...........................] - ETA: 20s - loss: 5.1721 - acc: 0.0625
1152/8000 [===>..........................] - ETA: 19s - loss: 5.1596 - acc: 0.0651
1280/8000 [===>..........................] - ETA: 19s - loss: 5.1746 - acc: 0.0641
1408/8000 [====>.........................] - ETA: 18s - loss: 5.2098 - acc: 0.0618
1536/8000 [====>.........................] - ETA: 18s - loss: 5.2421 - acc: 0.0592
1664/8000 [=====>........................] - ETA: 18s - loss: 5.2402 - acc: 0.0565
1792/8000 [=====>........................] - ETA: 17s - loss: 5.2467 - acc: 0.0541
1920/8000 [======>.......................] - ETA: 17s - loss: 5.2402 - acc: 0.0542
2048/8000 [======>.......................] - ETA: 17s - loss: 5.2222 - acc: 0.0562
2176/8000 [=======>......................] - ETA: 16s - loss: 5.2280 - acc: 0.0561
2304/8000 [=======>......................] - ETA: 16s - loss: 5.2217 - acc: 0.0551
2432/8000 [========>.....................] - ETA: 16s - loss: 5.2270 - acc: 0.0547
2560/8000 [========>.....................] - ETA: 15s - loss: 5.2250 - acc: 0.0543
2688/8000 [=========>....................] - ETA: 15s - loss: 5.2295 - acc: 0.0539
2816/8000 [=========>....................] - ETA: 14s - loss: 5.2315 - acc: 0.0533
2944/8000 [==========>...................] - ETA: 14s - loss: 5.2340 - acc: 0.0554
3072/8000 [==========>...................] - ETA: 14s - loss: 5.2316 - acc: 0.0560
3200/8000 [===========>..................] - ETA: 13s - loss: 5.2323 - acc: 0.0569
3328/8000 [===========>..................] - ETA: 13s - loss: 5.2259 - acc: 0.0577
3456/8000 [===========>..................] - ETA: 13s - loss: 5.2217 - acc: 0.0567
3584/8000 [============>.................] - ETA: 12s - loss: 5.2273 - acc: 0.0561
3712/8000 [============>.................] - ETA: 12s - loss: 5.2368 - acc: 0.0550
3840/8000 [=============>................] - ETA: 11s - loss: 5.2360 - acc: 0.0547
3968/8000 [=============>................] - ETA: 11s - loss: 5.2380 - acc: 0.0542
4096/8000 [==============>...............] - ETA: 11s - loss: 5.2237 - acc: 0.0554
4224/8000 [==============>...............] - ETA: 10s - loss: 5.2192 - acc: 0.0554
4352/8000 [===============>..............] - ETA: 10s - loss: 5.2146 - acc: 0.0558
4480/8000 [===============>..............] - ETA: 10s - loss: 5.2193 - acc: 0.0551
4608/8000 [================>.............] - ETA: 9s - loss: 5.2244 - acc: 0.0543
4736/8000 [================>.............] - ETA: 9s - loss: 5.2246 - acc: 0.0536
4864/8000 [=================>............] - ETA: 9s - loss: 5.2332 - acc: 0.0535
4992/8000 [=================>............] - ETA: 8s - loss: 5.2372 - acc: 0.0527
5120/8000 [==================>...........] - ETA: 8s - loss: 5.2377 - acc: 0.0527
5248/8000 [==================>...........] - ETA: 7s - loss: 5.2357 - acc: 0.0528
5376/8000 [===================>..........] - ETA: 7s - loss: 5.2353 - acc: 0.0526
5504/8000 [===================>..........] - ETA: 7s - loss: 5.2333 - acc: 0.0520
5632/8000 [====================>.........] - ETA: 6s - loss: 5.2313 - acc: 0.0520
5760/8000 [====================>.........] - ETA: 6s - loss: 5.2304 - acc: 0.0519
5888/8000 [=====================>........] - ETA: 6s - loss: 5.2339 - acc: 0.0520
6016/8000 [=====================>........] - ETA: 5s - loss: 5.2331 - acc: 0.0520
6144/8000 [======================>.......] - ETA: 5s - loss: 5.2354 - acc: 0.0521
6272/8000 [======================>.......] - ETA: 5s - loss: 5.2366 - acc: 0.0515
6400/8000 [=======================>......] - ETA: 4s - loss: 5.2383 - acc: 0.0511
6528/8000 [=======================>......] - ETA: 4s - loss: 5.2349 - acc: 0.0513
6656/8000 [=======================>......] - ETA: 3s - loss: 5.2425 - acc: 0.0509
6784/8000 [========================>.....] - ETA: 3s - loss: 5.2475 - acc: 0.0506
6912/8000 [========================>.....] - ETA: 3s - loss: 5.2463 - acc: 0.0512
7040/8000 [=========================>....] - ETA: 2s - loss: 5.2526 - acc: 0.0511
7168/8000 [=========================>....] - ETA: 2s - loss: 5.2526 - acc: 0.0516
7296/8000 [==========================>...] - ETA: 2s - loss: 5.2541 - acc: 0.0517
7424/8000 [==========================>...] - ETA: 1s - loss: 5.2585 - acc: 0.0512
7552/8000 [===========================>..] - ETA: 1s - loss: 5.2571 - acc: 0.0516
7680/8000 [===========================>..] - ETA: 0s - loss: 5.2601 - acc: 0.0516
7808/8000 [============================>.] - ETA: 0s - loss: 5.2633 - acc: 0.0511
7936/8000 [============================>.] - ETA: 0s - loss: 5.2679 - acc: 0.0507
8000/8000 [==============================] - 25s 3ms/step - loss: 5.2693 - acc: 0.0504 - val_loss: 6.8389 - val_acc: 0.0100
End: Sat Apr 13 18:42:29 EDT 2019


Start: Sun Apr 14 15:07:57 EDT 2019
End: Sun Apr 14 15:08:05 EDT 2019


Start: Sun Apr 14 15:08:33 EDT 2019
Indexing word vectors.
Found 400000 word vectors.
Found 171952 unique tokens.
Shape of data tensor: (100000, 1000)
Shape of label tensor: (100000, 1013)
Preparing embedding matrix.
End: Sun Apr 14 15:09:33 EDT 2019


Start: Sun Apr 14 15:46:01 EDT 2019
Indexing word vectors.
Found 400000 word vectors.
Found 171952 unique tokens.
Shape of data tensor: (100000, 1000)
Shape of label tensor: (100000, 1013)
Preparing embedding matrix.
Training model.
Train on 80000 samples, validate on 20000 samples
Epoch 1/10

80000/80000 [==============================] - 292s 4ms/step - loss: 6.6835 - acc: 0.0038 - val_loss: 6.2204 - val_acc: 0.0090
Epoch 2/10

80000/80000 [==============================] - 297s 4ms/step - loss: 5.7205 - acc: 0.0241 - val_loss: 5.3182 - val_acc: 0.0396
Epoch 3/10

80000/80000 [==============================] - 239s 3ms/step - loss: 4.9845 - acc: 0.0649 - val_loss: 4.9162 - val_acc: 0.0800
Epoch 4/10

80000/80000 [==============================] - 230s 3ms/step - loss: 4.4955 - acc: 0.1154 - val_loss: 4.6151 - val_acc: 0.1147
Epoch 5/10

80000/80000 [==============================] - 232s 3ms/step - loss: 4.1188 - acc: 0.1619 - val_loss: 4.3786 - val_acc: 0.1505
Epoch 6/10

80000/80000 [==============================] - 230s 3ms/step - loss: 3.8145 - acc: 0.2053 - val_loss: 4.2925 - val_acc: 0.1668
Epoch 7/10

80000/80000 [==============================] - 229s 3ms/step - loss: 3.5598 - acc: 0.2418 - val_loss: 4.2722 - val_acc: 0.1794
Epoch 8/10

80000/80000 [==============================] - 226s 3ms/step - loss: 3.3342 - acc: 0.2766 - val_loss: 4.2164 - val_acc: 0.1927
Epoch 9/10

80000/80000 [==============================] - 228s 3ms/step - loss: 3.1402 - acc: 0.3094 - val_loss: 4.2739 - val_acc: 0.2006
Epoch 10/10

80000/80000 [==============================] - 233s 3ms/step - loss: 2.9678 - acc: 0.3360 - val_loss: 4.3005 - val_acc: 0.1988
End: Sun Apr 14 16:27:49 EDT 2019


Start: Sun Apr 14 20:27:47 EDT 2019
Indexing word vectors.
End: Sun Apr 14 20:28:10 EDT 2019


Start: Sun Apr 14 20:29:08 EDT 2019
Indexing word vectors.
Found 400000 word vectors.
Found 171952 unique tokens.
Shape of data tensor: (100000, 1000)
Shape of label tensor: (100000, 1013)

80000/80000 [==============================] - 70s 881us/step

20000/20000 [==============================] - 18s 887us/step
End: Sun Apr 14 20:31:42 EDT 2019


Start: Sun Apr 14 20:34:45 EDT 2019
Indexing word vectors.
Found 400000 word vectors.
Found 171952 unique tokens.
Shape of data tensor: (100000, 1000)
Shape of label tensor: (100000, 1013)

80000/80000 [==============================] - 71s 892us/step
Test set: acc: 35.19

20000/20000 [==============================] - 18s 888us/step
Validation set: acc: 34.86
End: Sun Apr 14 20:37:19 EDT 2019


Start: Sun Apr 14 20:40:07 EDT 2019
Indexing word vectors.
Found 400000 word vectors.
Found 171952 unique tokens.
Shape of data tensor: (100000, 1000)
Shape of label tensor: (100000, 1013)

80000/80000 [==============================] - 73s 910us/step
Training set: ['loss', 'acc', 'top_k_categorical_accuracy']: [3.0125362129211424, 0.35085, 0.600325]

20000/20000 [==============================] - 18s 900us/step
Validation set: ['loss', 'acc', 'top_k_categorical_accuracy']: [2.9885347793579102, 0.3528, 0.6031]
End: Sun Apr 14 20:42:44 EDT 2019


Start: Sun Apr 14 20:52:40 EDT 2019
End: Sun Apr 14 20:52:44 EDT 2019


Start: Sun Apr 14 20:53:01 EDT 2019
Indexing word vectors.
Found 2469 unique tokens.
Shape of data tensor: (100, 1000)
Shape of label tensor: (100, 96)
Preparing embedding matrix.
Found 400000 word vectors.
End: Sun Apr 14 20:53:30 EDT 2019


Start: Sun Apr 14 20:54:18 EDT 2019
Indexing word vectors.
Found 2469 unique tokens.
Shape of data tensor: (100, 1000)
Shape of label tensor: (100, 96)
Preparing embedding matrix.
Found 400000 word vectors.
Training model.
Train on 80 samples, validate on 20 samples
Epoch 1/10

80/80 [==============================] - 1s 8ms/step - loss: 4.6201 - acc: 0.0000e+00 - top_k_categorical_accuracy: 0.0375 - val_loss: 4.7816 - val_acc: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00
Epoch 2/10

80/80 [==============================] - 0s 3ms/step - loss: 4.5267 - acc: 0.0125 - top_k_categorical_accuracy: 0.0875 - val_loss: 4.9883 - val_acc: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00
Epoch 3/10

80/80 [==============================] - 0s 3ms/step - loss: 4.3981 - acc: 0.0500 - top_k_categorical_accuracy: 0.1875 - val_loss: 5.1605 - val_acc: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00
Epoch 4/10

80/80 [==============================] - 0s 3ms/step - loss: 4.2639 - acc: 0.0375 - top_k_categorical_accuracy: 0.1625 - val_loss: 5.6125 - val_acc: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00
Epoch 5/10

80/80 [==============================] - 0s 3ms/step - loss: 4.1818 - acc: 0.0625 - top_k_categorical_accuracy: 0.1875 - val_loss: 5.4985 - val_acc: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00
Epoch 6/10

80/80 [==============================] - 0s 3ms/step - loss: 4.0677 - acc: 0.0375 - top_k_categorical_accuracy: 0.2000 - val_loss: 6.0681 - val_acc: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00
Epoch 7/10

80/80 [==============================] - 0s 3ms/step - loss: 3.8661 - acc: 0.1125 - top_k_categorical_accuracy: 0.3000 - val_loss: 6.0847 - val_acc: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00
Epoch 8/10

80/80 [==============================] - 0s 3ms/step - loss: 3.7472 - acc: 0.1625 - top_k_categorical_accuracy: 0.3750 - val_loss: 6.6604 - val_acc: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00
Epoch 9/10

80/80 [==============================] - 0s 3ms/step - loss: 3.5998 - acc: 0.1750 - top_k_categorical_accuracy: 0.4000 - val_loss: 7.3728 - val_acc: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00
Epoch 10/10

80/80 [==============================] - 0s 3ms/step - loss: 3.4081 - acc: 0.1000 - top_k_categorical_accuracy: 0.4625 - val_loss: 7.3995 - val_acc: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00
End: Sun Apr 14 20:54:52 EDT 2019


Start: Sun Apr 14 20:57:03 EDT 2019
Indexing word vectors.
Found 171952 unique tokens.
Shape of data tensor: (100000, 1000)
Shape of label tensor: (100000, 1013)
Preparing embedding matrix.
Found 400000 word vectors.
Training model.
Train on 80000 samples, validate on 20000 samples
Epoch 1/100

80000/80000 [==============================] - 221s 3ms/step - loss: 6.6233 - acc: 0.0040 - top_k_categorical_accuracy: 0.0174 - val_loss: 6.0892 - val_acc: 0.0098 - val_top_k_categorical_accuracy: 0.0420
Epoch 2/100

80000/80000 [==============================] - 221s 3ms/step - loss: 5.5705 - acc: 0.0277 - top_k_categorical_accuracy: 0.0998 - val_loss: 5.2726 - val_acc: 0.0420 - val_top_k_categorical_accuracy: 0.1362
Epoch 3/100

80000/80000 [==============================] - 223s 3ms/step - loss: 4.9008 - acc: 0.0701 - top_k_categorical_accuracy: 0.2075 - val_loss: 4.7563 - val_acc: 0.0891 - val_top_k_categorical_accuracy: 0.2454
Epoch 4/100

80000/80000 [==============================] - 225s 3ms/step - loss: 4.4173 - acc: 0.1218 - top_k_categorical_accuracy: 0.3048 - val_loss: 4.5053 - val_acc: 0.1213 - val_top_k_categorical_accuracy: 0.3004
Epoch 5/100

80000/80000 [==============================] - 224s 3ms/step - loss: 4.0409 - acc: 0.1718 - top_k_categorical_accuracy: 0.3845 - val_loss: 4.3248 - val_acc: 0.1529 - val_top_k_categorical_accuracy: 0.3457
Epoch 6/100

80000/80000 [==============================] - 222s 3ms/step - loss: 3.7427 - acc: 0.2126 - top_k_categorical_accuracy: 0.4449 - val_loss: 4.2171 - val_acc: 0.1756 - val_top_k_categorical_accuracy: 0.3790
Epoch 7/100

80000/80000 [==============================] - 221s 3ms/step - loss: 3.4914 - acc: 0.2529 - top_k_categorical_accuracy: 0.4950 - val_loss: 4.3441 - val_acc: 0.1744 - val_top_k_categorical_accuracy: 0.3719
Epoch 8/100

80000/80000 [==============================] - 221s 3ms/step - loss: 3.2783 - acc: 0.2870 - top_k_categorical_accuracy: 0.5333 - val_loss: 4.1594 - val_acc: 0.1989 - val_top_k_categorical_accuracy: 0.4041
Epoch 9/100

80000/80000 [==============================] - 220s 3ms/step - loss: 3.0868 - acc: 0.3154 - top_k_categorical_accuracy: 0.5703 - val_loss: 4.2744 - val_acc: 0.1933 - val_top_k_categorical_accuracy: 0.3917
Epoch 10/100

80000/80000 [==============================] - 220s 3ms/step - loss: 2.9134 - acc: 0.3444 - top_k_categorical_accuracy: 0.6034 - val_loss: 4.2295 - val_acc: 0.2097 - val_top_k_categorical_accuracy: 0.4143
Epoch 11/100

80000/80000 [==============================] - 222s 3ms/step - loss: 2.7519 - acc: 0.3720 - top_k_categorical_accuracy: 0.6330 - val_loss: 4.3870 - val_acc: 0.2099 - val_top_k_categorical_accuracy: 0.4190
Epoch 12/100

80000/80000 [==============================] - 222s 3ms/step - loss: 2.6156 - acc: 0.3946 - top_k_categorical_accuracy: 0.6581 - val_loss: 4.4451 - val_acc: 0.2042 - val_top_k_categorical_accuracy: 0.4023
Epoch 13/100

80000/80000 [==============================] - 233s 3ms/step - loss: 2.4792 - acc: 0.4192 - top_k_categorical_accuracy: 0.6820 - val_loss: 4.5719 - val_acc: 0.2061 - val_top_k_categorical_accuracy: 0.4101
Epoch 14/100

80000/80000 [==============================] - 227s 3ms/step - loss: 2.3596 - acc: 0.4394 - top_k_categorical_accuracy: 0.7028 - val_loss: 4.7459 - val_acc: 0.1976 - val_top_k_categorical_accuracy: 0.4000
Epoch 15/100

80000/80000 [==============================] - 243s 3ms/step - loss: 2.2500 - acc: 0.4592 - top_k_categorical_accuracy: 0.7230 - val_loss: 4.8310 - val_acc: 0.2021 - val_top_k_categorical_accuracy: 0.4004
Epoch 16/100

80000/80000 [==============================] - 242s 3ms/step - loss: 2.1556 - acc: 0.4772 - top_k_categorical_accuracy: 0.7412 - val_loss: 4.9553 - val_acc: 0.2046 - val_top_k_categorical_accuracy: 0.4051
Epoch 17/100

80000/80000 [==============================] - 243s 3ms/step - loss: 2.0611 - acc: 0.4944 - top_k_categorical_accuracy: 0.7579 - val_loss: 5.1070 - val_acc: 0.1941 - val_top_k_categorical_accuracy: 0.3936
Epoch 18/100

80000/80000 [==============================] - 242s 3ms/step - loss: 1.9717 - acc: 0.5104 - top_k_categorical_accuracy: 0.7723 - val_loss: 5.3036 - val_acc: 0.2042 - val_top_k_categorical_accuracy: 0.4021
Epoch 19/100

80000/80000 [==============================] - 240s 3ms/step - loss: 1.8947 - acc: 0.5250 - top_k_categorical_accuracy: 0.7874 - val_loss: 5.4930 - val_acc: 0.1991 - val_top_k_categorical_accuracy: 0.3966
Epoch 20/100

80000/80000 [==============================] - 238s 3ms/step - loss: 1.8224 - acc: 0.5395 - top_k_categorical_accuracy: 0.7985 - val_loss: 5.7421 - val_acc: 0.1953 - val_top_k_categorical_accuracy: 0.3928
Epoch 21/100

80000/80000 [==============================] - 237s 3ms/step - loss: 1.7553 - acc: 0.5534 - top_k_categorical_accuracy: 0.8112 - val_loss: 5.7278 - val_acc: 0.1931 - val_top_k_categorical_accuracy: 0.3948
Epoch 22/100

80000/80000 [==============================] - 236s 3ms/step - loss: 1.6928 - acc: 0.5660 - top_k_categorical_accuracy: 0.8206 - val_loss: 5.8661 - val_acc: 0.1908 - val_top_k_categorical_accuracy: 0.3825
Epoch 23/100

80000/80000 [==============================] - 240s 3ms/step - loss: 1.6367 - acc: 0.5775 - top_k_categorical_accuracy: 0.8308 - val_loss: 6.0282 - val_acc: 0.1882 - val_top_k_categorical_accuracy: 0.3840
Epoch 24/100

80000/80000 [==============================] - 237s 3ms/step - loss: 1.5811 - acc: 0.5891 - top_k_categorical_accuracy: 0.8403 - val_loss: 6.3243 - val_acc: 0.1915 - val_top_k_categorical_accuracy: 0.3886
Epoch 25/100

80000/80000 [==============================] - 235s 3ms/step - loss: 1.5333 - acc: 0.5996 - top_k_categorical_accuracy: 0.8481 - val_loss: 6.1779 - val_acc: 0.1834 - val_top_k_categorical_accuracy: 0.3750
Epoch 26/100

80000/80000 [==============================] - 255s 3ms/step - loss: 1.4889 - acc: 0.6095 - top_k_categorical_accuracy: 0.8551 - val_loss: 6.5050 - val_acc: 0.1883 - val_top_k_categorical_accuracy: 0.3822
Epoch 27/100

80000/80000 [==============================] - 266s 3ms/step - loss: 1.4466 - acc: 0.6185 - top_k_categorical_accuracy: 0.8629 - val_loss: 6.6663 - val_acc: 0.1800 - val_top_k_categorical_accuracy: 0.3674
Epoch 28/100

80000/80000 [==============================] - 259s 3ms/step - loss: 1.4101 - acc: 0.6263 - top_k_categorical_accuracy: 0.8694 - val_loss: 6.7527 - val_acc: 0.1837 - val_top_k_categorical_accuracy: 0.3735
Epoch 29/100

80000/80000 [==============================] - 272s 3ms/step - loss: 1.3773 - acc: 0.6326 - top_k_categorical_accuracy: 0.8748 - val_loss: 6.9058 - val_acc: 0.1862 - val_top_k_categorical_accuracy: 0.3782
Epoch 30/100

80000/80000 [==============================] - 253s 3ms/step - loss: 1.3401 - acc: 0.6404 - top_k_categorical_accuracy: 0.8797 - val_loss: 7.1382 - val_acc: 0.1855 - val_top_k_categorical_accuracy: 0.3767
Epoch 31/100

80000/80000 [==============================] - 247s 3ms/step - loss: 1.3198 - acc: 0.6465 - top_k_categorical_accuracy: 0.8853 - val_loss: 7.1970 - val_acc: 0.1823 - val_top_k_categorical_accuracy: 0.3719
Epoch 32/100

80000/80000 [==============================] - 241s 3ms/step - loss: 1.2913 - acc: 0.6535 - top_k_categorical_accuracy: 0.8886 - val_loss: 7.1783 - val_acc: 0.1801 - val_top_k_categorical_accuracy: 0.3652
Epoch 33/100

80000/80000 [==============================] - 244s 3ms/step - loss: 1.2608 - acc: 0.6585 - top_k_categorical_accuracy: 0.8938 - val_loss: 7.4171 - val_acc: 0.1772 - val_top_k_categorical_accuracy: 0.3654
Epoch 34/100

80000/80000 [==============================] - 245s 3ms/step - loss: 1.2429 - acc: 0.6660 - top_k_categorical_accuracy: 0.8966 - val_loss: 7.3468 - val_acc: 0.1793 - val_top_k_categorical_accuracy: 0.3670
Epoch 35/100

80000/80000 [==============================] - 245s 3ms/step - loss: 1.2206 - acc: 0.6705 - top_k_categorical_accuracy: 0.9003 - val_loss: 7.7206 - val_acc: 0.1839 - val_top_k_categorical_accuracy: 0.3712
Epoch 36/100

80000/80000 [==============================] - 238s 3ms/step - loss: 1.1945 - acc: 0.6777 - top_k_categorical_accuracy: 0.9044 - val_loss: 7.7350 - val_acc: 0.1785 - val_top_k_categorical_accuracy: 0.3648
Epoch 37/100

80000/80000 [==============================] - 241s 3ms/step - loss: 1.1816 - acc: 0.6795 - top_k_categorical_accuracy: 0.9073 - val_loss: 7.8643 - val_acc: 0.1794 - val_top_k_categorical_accuracy: 0.3625
Epoch 38/100

80000/80000 [==============================] - 231s 3ms/step - loss: 1.1680 - acc: 0.6851 - top_k_categorical_accuracy: 0.9097 - val_loss: 8.0242 - val_acc: 0.1809 - val_top_k_categorical_accuracy: 0.3685
Epoch 39/100

80000/80000 [==============================] - 243s 3ms/step - loss: 1.1455 - acc: 0.6898 - top_k_categorical_accuracy: 0.9136 - val_loss: 8.0614 - val_acc: 0.1764 - val_top_k_categorical_accuracy: 0.3644
Epoch 40/100

80000/80000 [==============================] - 258s 3ms/step - loss: 1.1306 - acc: 0.6925 - top_k_categorical_accuracy: 0.9152 - val_loss: 8.1652 - val_acc: 0.1784 - val_top_k_categorical_accuracy: 0.3674
Epoch 41/100

80000/80000 [==============================] - 265s 3ms/step - loss: 1.1163 - acc: 0.6969 - top_k_categorical_accuracy: 0.9178 - val_loss: 8.1587 - val_acc: 0.1764 - val_top_k_categorical_accuracy: 0.3659
Epoch 42/100

80000/80000 [==============================] - 270s 3ms/step - loss: 1.1088 - acc: 0.7013 - top_k_categorical_accuracy: 0.9188 - val_loss: 8.3066 - val_acc: 0.1741 - val_top_k_categorical_accuracy: 0.3579
Epoch 43/100

80000/80000 [==============================] - 260s 3ms/step - loss: 1.0948 - acc: 0.7034 - top_k_categorical_accuracy: 0.9227 - val_loss: 8.3521 - val_acc: 0.1719 - val_top_k_categorical_accuracy: 0.3529
Epoch 44/100

80000/80000 [==============================] - 256s 3ms/step - loss: 1.0910 - acc: 0.7053 - top_k_categorical_accuracy: 0.9235 - val_loss: 8.4043 - val_acc: 0.1704 - val_top_k_categorical_accuracy: 0.3555
Epoch 45/100

80000/80000 [==============================] - 248s 3ms/step - loss: 1.0773 - acc: 0.7081 - top_k_categorical_accuracy: 0.9262 - val_loss: 8.5451 - val_acc: 0.1749 - val_top_k_categorical_accuracy: 0.3617
Epoch 46/100

80000/80000 [==============================] - 248s 3ms/step - loss: 1.0668 - acc: 0.7108 - top_k_categorical_accuracy: 0.9266 - val_loss: 8.5922 - val_acc: 0.1718 - val_top_k_categorical_accuracy: 0.3588
Epoch 47/100

80000/80000 [==============================] - 246s 3ms/step - loss: 1.0596 - acc: 0.7133 - top_k_categorical_accuracy: 0.9293 - val_loss: 8.5660 - val_acc: 0.1652 - val_top_k_categorical_accuracy: 0.3448
Epoch 48/100

80000/80000 [==============================] - 246s 3ms/step - loss: 1.0497 - acc: 0.7182 - top_k_categorical_accuracy: 0.9305 - val_loss: 8.6918 - val_acc: 0.1758 - val_top_k_categorical_accuracy: 0.3589
Epoch 49/100

80000/80000 [==============================] - 248s 3ms/step - loss: 1.0363 - acc: 0.7221 - top_k_categorical_accuracy: 0.9315 - val_loss: 8.8265 - val_acc: 0.1732 - val_top_k_categorical_accuracy: 0.3564
Epoch 50/100

80000/80000 [==============================] - 248s 3ms/step - loss: 1.0340 - acc: 0.7208 - top_k_categorical_accuracy: 0.9330 - val_loss: 8.8335 - val_acc: 0.1741 - val_top_k_categorical_accuracy: 0.3563

End: Mon Apr 15 00:19:32 EDT 2019

Start: Mon Apr 15 00:21:23 EDT 2019
Indexing word vectors.
Found 758611 unique tokens.
Shape of data tensor: (1000000, 1000)
Shape of label tensor: (1000000, 1013)
Preparing embedding matrix.
Found 400000 word vectors.
Training model.
Train on 800000 samples, validate on 200000 samples
Epoch 1/10

800000/800000 [==============================] - 2393s 3ms/step - loss: 4.4800 - acc: 0.1578 - top_k_categorical_accuracy: 0.3261 - val_loss: 3.5507 - val_acc: 0.2854 - val_top_k_categorical_accuracy: 0.5060
Epoch 2/10

800000/800000 [==============================] - 2366s 3ms/step - loss: 3.2971 - acc: 0.3336 - top_k_categorical_accuracy: 0.5512 - val_loss: 3.2049 - val_acc: 0.3573 - val_top_k_categorical_accuracy: 0.5729
Epoch 3/10

800000/800000 [==============================] - 2340s 3ms/step - loss: 3.0784 - acc: 0.3789 - top_k_categorical_accuracy: 0.5915 - val_loss: 3.0847 - val_acc: 0.3830 - val_top_k_categorical_accuracy: 0.5927
Epoch 4/10

800000/800000 [==============================] - 2305s 3ms/step - loss: 2.9990 - acc: 0.3969 - top_k_categorical_accuracy: 0.6060 - val_loss: 3.0540 - val_acc: 0.3966 - val_top_k_categorical_accuracy: 0.6040
Epoch 5/10

800000/800000 [==============================] - 2279s 3ms/step - loss: 2.9618 - acc: 0.4070 - top_k_categorical_accuracy: 0.6128 - val_loss: 3.0629 - val_acc: 0.4017 - val_top_k_categorical_accuracy: 0.6045
Epoch 6/10

800000/800000 [==============================] - 2281s 3ms/step - loss: 2.9456 - acc: 0.4111 - top_k_categorical_accuracy: 0.6162 - val_loss: 3.0671 - val_acc: 0.3986 - val_top_k_categorical_accuracy: 0.6020
Epoch 7/10

800000/800000 [==============================] - 2277s 3ms/step - loss: 2.9400 - acc: 0.4144 - top_k_categorical_accuracy: 0.6177 - val_loss: 3.1193 - val_acc: 0.3977 - val_top_k_categorical_accuracy: 0.5971
Epoch 8/10

800000/800000 [==============================] - 2285s 3ms/step - loss: 2.9385 - acc: 0.4151 - top_k_categorical_accuracy: 0.6186 - val_loss: 3.1037 - val_acc: 0.3915 - val_top_k_categorical_accuracy: 0.5941
Epoch 9/10

800000/800000 [==============================] - 2288s 3ms/step - loss: 2.9417 - acc: 0.4158 - top_k_categorical_accuracy: 0.6179 - val_loss: 3.0771 - val_acc: 0.4081 - val_top_k_categorical_accuracy: 0.6062
Epoch 10/10

800000/800000 [==============================] - 2298s 3ms/step - loss: 2.9480 - acc: 0.4157 - top_k_categorical_accuracy: 0.6179 - val_loss: 3.0720 - val_acc: 0.3990 - val_top_k_categorical_accuracy: 0.6005
End: Mon Apr 15 06:52:13 EDT 2019

Start: Mon Apr 15 19:05:20 EDT 2019
Indexing word vectors.
Found 758611 unique tokens.
Shape of data tensor: (1000000, 1000)
Shape of label tensor: (1000000, 1013)
Preparing embedding matrix.
Found 400000 word vectors.
Training model.
Train on 800000 samples, validate on 200000 samples
Epoch 1/2

800000/800000 [==============================] - 10489s 13ms/step - loss: 5.7267 - acc: 0.1127 - top_k_categorical_accuracy: 0.2124 - val_loss: 3.7557 - val_acc: 0.3839 - val_top_k_categorical_accuracy: 0.5714

Epoch 00001: val_acc improved from -inf to 0.38392, saving model to models/CNN-multi-channel-2-epochs-1000000-rows-300-dim-2019-04-15-19:11:09-297146.h5
Epoch 2/2

800000/800000 [==============================] - 10644s 13ms/step - loss: 3.1490 - acc: 0.4168 - top_k_categorical_accuracy: 0.6068 - val_loss: 2.3781 - val_acc: 0.5586 - val_top_k_categorical_accuracy: 0.7370

Epoch 00002: val_acc improved from 0.38392 to 0.55859, saving model to models/CNN-multi-channel-2-epochs-1000000-rows-300-dim-2019-04-15-19:11:09-297146.h5

Details:

Epoch 1/2

 12800/800000 [..............................] - ETA: 2:41:13 - loss: 7.2658 - acc: 8.5938e-04 - top_k_categorical_accuracy: 0.0049
 25600/800000 [..............................] - ETA: 2:37:23 - loss: 7.1703 - acc: 0.0010 - top_k_categorical_accuracy: 0.0052
 38400/800000 [>.............................] - ETA: 2:34:38 - loss: 7.1234 - acc: 0.0011 - top_k_categorical_accuracy: 0.0053
 51200/800000 [>.............................] - ETA: 2:31:26 - loss: 7.0910 - acc: 0.0011 - top_k_categorical_accuracy: 0.0058
 64000/800000 [=>............................] - ETA: 2:28:23 - loss: 7.0666 - acc: 0.0013 - top_k_categorical_accuracy: 0.0060
 76800/800000 [=>............................] - ETA: 2:25:32 - loss: 7.0481 - acc: 0.0013 - top_k_categorical_accuracy: 0.0065
 89600/800000 [==>...........................] - ETA: 2:22:47 - loss: 7.0304 - acc: 0.0015 - top_k_categorical_accuracy: 0.0072
102400/800000 [==>...........................] - ETA: 2:20:17 - loss: 7.0140 - acc: 0.0018 - top_k_categorical_accuracy: 0.0080
115200/800000 [===>..........................] - ETA: 2:17:52 - loss: 6.9987 - acc: 0.0022 - top_k_categorical_accuracy: 0.0088
128000/800000 [===>..........................] - ETA: 2:15:20 - loss: 6.9844 - acc: 0.0026 - top_k_categorical_accuracy: 0.0098
140800/800000 [====>.........................] - ETA: 2:12:48 - loss: 6.9714 - acc: 0.0029 - top_k_categorical_accuracy: 0.0108
153600/800000 [====>.........................] - ETA: 2:10:04 - loss: 6.9584 - acc: 0.0034 - top_k_categorical_accuracy: 0.0122
166400/800000 [=====>........................] - ETA: 2:07:23 - loss: 6.9447 - acc: 0.0040 - top_k_categorical_accuracy: 0.0137
179200/800000 [=====>........................] - ETA: 2:04:41 - loss: 6.9315 - acc: 0.0045 - top_k_categorical_accuracy: 0.0152
192000/800000 [======>.......................] - ETA: 2:02:12 - loss: 6.9178 - acc: 0.0053 - top_k_categorical_accuracy: 0.0169
204800/800000 [======>.......................] - ETA: 1:59:37 - loss: 6.9039 - acc: 0.0061 - top_k_categorical_accuracy: 0.0189
217600/800000 [=======>......................] - ETA: 1:57:01 - loss: 6.8900 - acc: 0.0071 - top_k_categorical_accuracy: 0.0213
230400/800000 [=======>......................] - ETA: 1:54:21 - loss: 6.8754 - acc: 0.0081 - top_k_categorical_accuracy: 0.0240
243200/800000 [========>.....................] - ETA: 1:51:45 - loss: 6.8609 - acc: 0.0093 - top_k_categorical_accuracy: 0.0266
256000/800000 [========>.....................] - ETA: 1:49:12 - loss: 6.8460 - acc: 0.0106 - top_k_categorical_accuracy: 0.0295
268800/800000 [=========>....................] - ETA: 1:46:35 - loss: 6.8302 - acc: 0.0120 - top_k_categorical_accuracy: 0.0325
281600/800000 [=========>....................] - ETA: 1:44:00 - loss: 6.8140 - acc: 0.0134 - top_k_categorical_accuracy: 0.0357
294400/800000 [==========>...................] - ETA: 1:41:25 - loss: 6.7974 - acc: 0.0151 - top_k_categorical_accuracy: 0.0393
307200/800000 [==========>...................] - ETA: 1:38:52 - loss: 6.7801 - acc: 0.0168 - top_k_categorical_accuracy: 0.0427
320000/800000 [===========>..................] - ETA: 1:36:17 - loss: 6.7617 - acc: 0.0185 - top_k_categorical_accuracy: 0.0466
332800/800000 [===========>..................] - ETA: 1:33:43 - loss: 6.7422 - acc: 0.0203 - top_k_categorical_accuracy: 0.0502
345600/800000 [===========>..................] - ETA: 1:31:08 - loss: 6.7215 - acc: 0.0222 - top_k_categorical_accuracy: 0.0542
358400/800000 [============>.................] - ETA: 1:28:33 - loss: 6.7003 - acc: 0.0241 - top_k_categorical_accuracy: 0.0583
371200/800000 [============>.................] - ETA: 1:25:59 - loss: 6.6787 - acc: 0.0260 - top_k_categorical_accuracy: 0.0622
384000/800000 [=============>................] - ETA: 1:23:24 - loss: 6.6558 - acc: 0.0280 - top_k_categorical_accuracy: 0.0663
396800/800000 [=============>................] - ETA: 1:20:49 - loss: 6.6318 - acc: 0.0301 - top_k_categorical_accuracy: 0.0706
409600/800000 [==============>...............] - ETA: 1:18:15 - loss: 6.6070 - acc: 0.0322 - top_k_categorical_accuracy: 0.0748
422400/800000 [==============>...............] - ETA: 1:15:40 - loss: 6.5816 - acc: 0.0343 - top_k_categorical_accuracy: 0.0792
435200/800000 [===============>..............] - ETA: 1:13:06 - loss: 6.5554 - acc: 0.0365 - top_k_categorical_accuracy: 0.0836
448000/800000 [===============>..............] - ETA: 1:10:32 - loss: 6.5285 - acc: 0.0388 - top_k_categorical_accuracy: 0.0882
460800/800000 [================>.............] - ETA: 1:07:58 - loss: 6.5010 - acc: 0.0411 - top_k_categorical_accuracy: 0.0925
473600/800000 [================>.............] - ETA: 1:05:24 - loss: 6.4730 - acc: 0.0434 - top_k_categorical_accuracy: 0.0971
486400/800000 [=================>............] - ETA: 1:02:50 - loss: 6.4440 - acc: 0.0459 - top_k_categorical_accuracy: 0.1017
499200/800000 [=================>............] - ETA: 1:00:17 - loss: 6.4156 - acc: 0.0483 - top_k_categorical_accuracy: 0.1063
512000/800000 [==================>...........] - ETA: 57:42 - loss: 6.3859 - acc: 0.0508 - top_k_categorical_accuracy: 0.1110
524800/800000 [==================>...........] - ETA: 55:10 - loss: 6.3564 - acc: 0.0532 - top_k_categorical_accuracy: 0.1155
537600/800000 [===================>..........] - ETA: 52:36 - loss: 6.3265 - acc: 0.0558 - top_k_categorical_accuracy: 0.1203
550400/800000 [===================>..........] - ETA: 50:03 - loss: 6.2966 - acc: 0.0585 - top_k_categorical_accuracy: 0.1250
563200/800000 [====================>.........] - ETA: 47:31 - loss: 6.2670 - acc: 0.0610 - top_k_categorical_accuracy: 0.1295
576000/800000 [====================>.........] - ETA: 45:03 - loss: 6.2372 - acc: 0.0636 - top_k_categorical_accuracy: 0.1341
588800/800000 [=====================>........] - ETA: 42:34 - loss: 6.2073 - acc: 0.0663 - top_k_categorical_accuracy: 0.1388
601600/800000 [=====================>........] - ETA: 40:02 - loss: 6.1774 - acc: 0.0689 - top_k_categorical_accuracy: 0.1434
614400/800000 [======================>.......] - ETA: 37:30 - loss: 6.1478 - acc: 0.0717 - top_k_categorical_accuracy: 0.1480
627200/800000 [======================>.......] - ETA: 34:58 - loss: 6.1173 - acc: 0.0746 - top_k_categorical_accuracy: 0.1527
640000/800000 [=======================>......] - ETA: 32:25 - loss: 6.0871 - acc: 0.0774 - top_k_categorical_accuracy: 0.1573
652800/800000 [=======================>......] - ETA: 29:51 - loss: 6.0570 - acc: 0.0801 - top_k_categorical_accuracy: 0.1618
665600/800000 [=======================>......] - ETA: 27:18 - loss: 6.0273 - acc: 0.0830 - top_k_categorical_accuracy: 0.1665
678400/800000 [========================>.....] - ETA: 24:43 - loss: 5.9983 - acc: 0.0857 - top_k_categorical_accuracy: 0.1709
691200/800000 [========================>.....] - ETA: 22:09 - loss: 5.9690 - acc: 0.0885 - top_k_categorical_accuracy: 0.1754
704000/800000 [=========================>....] - ETA: 19:34 - loss: 5.9393 - acc: 0.0914 - top_k_categorical_accuracy: 0.1799
716800/800000 [=========================>....] - ETA: 16:58 - loss: 5.9106 - acc: 0.0941 - top_k_categorical_accuracy: 0.1841
729600/800000 [==========================>...] - ETA: 14:22 - loss: 5.8818 - acc: 0.0970 - top_k_categorical_accuracy: 0.1885
742400/800000 [==========================>...] - ETA: 11:45 - loss: 5.8532 - acc: 0.0998 - top_k_categorical_accuracy: 0.1930
755200/800000 [===========================>..] - ETA: 9:09 - loss: 5.8246 - acc: 0.1026 - top_k_categorical_accuracy: 0.1973
768000/800000 [===========================>..] - ETA: 6:32 - loss: 5.7965 - acc: 0.1054 - top_k_categorical_accuracy: 0.2016
780800/800000 [============================>.] - ETA: 3:55 - loss: 5.7686 - acc: 0.1083 - top_k_categorical_accuracy: 0.2059
793600/800000 [============================>.] - ETA: 1:18 - loss: 5.7407 - acc: 0.1112 - top_k_categorical_accuracy: 0.2102

Epoch 00001: val_acc improved from -inf to 0.38392, saving model to models/CNN-multi-channel-2-epochs-1000000-rows-300-dim-2019-04-15-19:11:09-297146.h5
Epoch 2/2

 12800/800000 [..............................] - ETA: 2:39:14 - loss: 3.9194 - acc: 0.3045 - top_k_categorical_accuracy: 0.4923
 25600/800000 [..............................] - ETA: 2:36:44 - loss: 3.9024 - acc: 0.3063 - top_k_categorical_accuracy: 0.4950
 38400/800000 [>.............................] - ETA: 2:34:21 - loss: 3.8904 - acc: 0.3093 - top_k_categorical_accuracy: 0.4970
 51200/800000 [>.............................] - ETA: 2:32:03 - loss: 3.8826 - acc: 0.3089 - top_k_categorical_accuracy: 0.4974
 64000/800000 [=>............................] - ETA: 2:30:13 - loss: 3.8602 - acc: 0.3128 - top_k_categorical_accuracy: 0.5007
 76800/800000 [=>............................] - ETA: 2:28:18 - loss: 3.8430 - acc: 0.3147 - top_k_categorical_accuracy: 0.5039
 89600/800000 [==>...........................] - ETA: 2:25:59 - loss: 3.8273 - acc: 0.3165 - top_k_categorical_accuracy: 0.5062
102400/800000 [==>...........................] - ETA: 2:23:48 - loss: 3.8097 - acc: 0.3192 - top_k_categorical_accuracy: 0.5087
115200/800000 [===>..........................] - ETA: 2:21:16 - loss: 3.7919 - acc: 0.3217 - top_k_categorical_accuracy: 0.5113
128000/800000 [===>..........................] - ETA: 2:18:32 - loss: 3.7744 - acc: 0.3243 - top_k_categorical_accuracy: 0.5140
140800/800000 [====>.........................] - ETA: 2:15:48 - loss: 3.7570 - acc: 0.3266 - top_k_categorical_accuracy: 0.5165
153600/800000 [====>.........................] - ETA: 2:13:07 - loss: 3.7409 - acc: 0.3289 - top_k_categorical_accuracy: 0.5188
166400/800000 [=====>........................] - ETA: 2:10:23 - loss: 3.7255 - acc: 0.3314 - top_k_categorical_accuracy: 0.5212
179200/800000 [=====>........................] - ETA: 2:07:47 - loss: 3.7089 - acc: 0.3336 - top_k_categorical_accuracy: 0.5235
192000/800000 [======>.......................] - ETA: 2:05:18 - loss: 3.6904 - acc: 0.3365 - top_k_categorical_accuracy: 0.5264
204800/800000 [======>.......................] - ETA: 2:02:43 - loss: 3.6743 - acc: 0.3388 - top_k_categorical_accuracy: 0.5287
217600/800000 [=======>......................] - ETA: 2:00:05 - loss: 3.6606 - acc: 0.3408 - top_k_categorical_accuracy: 0.5306
230400/800000 [=======>......................] - ETA: 1:57:33 - loss: 3.6431 - acc: 0.3432 - top_k_categorical_accuracy: 0.5332
243200/800000 [========>.....................] - ETA: 1:54:59 - loss: 3.6284 - acc: 0.3450 - top_k_categorical_accuracy: 0.5353
256000/800000 [========>.....................] - ETA: 1:52:26 - loss: 3.6161 - acc: 0.3469 - top_k_categorical_accuracy: 0.5371
268800/800000 [=========>....................] - ETA: 1:49:52 - loss: 3.6010 - acc: 0.3491 - top_k_categorical_accuracy: 0.5395
281600/800000 [=========>....................] - ETA: 1:47:19 - loss: 3.5852 - acc: 0.3515 - top_k_categorical_accuracy: 0.5421
294400/800000 [==========>...................] - ETA: 1:44:40 - loss: 3.5708 - acc: 0.3536 - top_k_categorical_accuracy: 0.5442
307200/800000 [==========>...................] - ETA: 1:42:02 - loss: 3.5561 - acc: 0.3558 - top_k_categorical_accuracy: 0.5463
320000/800000 [===========>..................] - ETA: 1:39:26 - loss: 3.5435 - acc: 0.3576 - top_k_categorical_accuracy: 0.5483
332800/800000 [===========>..................] - ETA: 1:36:48 - loss: 3.5300 - acc: 0.3596 - top_k_categorical_accuracy: 0.5501
345600/800000 [===========>..................] - ETA: 1:34:09 - loss: 3.5145 - acc: 0.3619 - top_k_categorical_accuracy: 0.5524
358400/800000 [============>.................] - ETA: 1:31:32 - loss: 3.5010 - acc: 0.3638 - top_k_categorical_accuracy: 0.5545
371200/800000 [============>.................] - ETA: 1:28:53 - loss: 3.4893 - acc: 0.3654 - top_k_categorical_accuracy: 0.5563
384000/800000 [=============>................] - ETA: 1:26:15 - loss: 3.4762 - acc: 0.3674 - top_k_categorical_accuracy: 0.5582
396800/800000 [=============>................] - ETA: 1:23:37 - loss: 3.4636 - acc: 0.3692 - top_k_categorical_accuracy: 0.5600
409600/800000 [==============>...............] - ETA: 1:21:00 - loss: 3.4504 - acc: 0.3713 - top_k_categorical_accuracy: 0.5621
422400/800000 [==============>...............] - ETA: 1:18:21 - loss: 3.4383 - acc: 0.3731 - top_k_categorical_accuracy: 0.5640
435200/800000 [===============>..............] - ETA: 1:15:44 - loss: 3.4263 - acc: 0.3748 - top_k_categorical_accuracy: 0.5657
448000/800000 [===============>..............] - ETA: 1:13:03 - loss: 3.4142 - acc: 0.3765 - top_k_categorical_accuracy: 0.5674
460800/800000 [================>.............] - ETA: 1:10:24 - loss: 3.4025 - acc: 0.3782 - top_k_categorical_accuracy: 0.5691
473600/800000 [================>.............] - ETA: 1:07:43 - loss: 3.3909 - acc: 0.3799 - top_k_categorical_accuracy: 0.5708
486400/800000 [=================>............] - ETA: 1:05:03 - loss: 3.3803 - acc: 0.3815 - top_k_categorical_accuracy: 0.5723
499200/800000 [=================>............] - ETA: 1:02:23 - loss: 3.3693 - acc: 0.3830 - top_k_categorical_accuracy: 0.5740
512000/800000 [==================>...........] - ETA: 59:43 - loss: 3.3587 - acc: 0.3845 - top_k_categorical_accuracy: 0.5756
524800/800000 [==================>...........] - ETA: 57:03 - loss: 3.3483 - acc: 0.3860 - top_k_categorical_accuracy: 0.5770
537600/800000 [===================>..........] - ETA: 54:24 - loss: 3.3380 - acc: 0.3876 - top_k_categorical_accuracy: 0.5785
550400/800000 [===================>..........] - ETA: 51:48 - loss: 3.3267 - acc: 0.3894 - top_k_categorical_accuracy: 0.5802
563200/800000 [====================>.........] - ETA: 49:12 - loss: 3.3168 - acc: 0.3909 - top_k_categorical_accuracy: 0.5816
576000/800000 [====================>.........] - ETA: 46:34 - loss: 3.3069 - acc: 0.3923 - top_k_categorical_accuracy: 0.5831
588800/800000 [=====================>........] - ETA: 43:56 - loss: 3.2968 - acc: 0.3939 - top_k_categorical_accuracy: 0.5846
601600/800000 [=====================>........] - ETA: 41:16 - loss: 3.2870 - acc: 0.3955 - top_k_categorical_accuracy: 0.5861
614400/800000 [======================>.......] - ETA: 38:36 - loss: 3.2777 - acc: 0.3969 - top_k_categorical_accuracy: 0.5876
627200/800000 [======================>.......] - ETA: 35:56 - loss: 3.2688 - acc: 0.3983 - top_k_categorical_accuracy: 0.5888
640000/800000 [=======================>......] - ETA: 33:16 - loss: 3.2590 - acc: 0.3998 - top_k_categorical_accuracy: 0.5903
652800/800000 [=======================>......] - ETA: 30:36 - loss: 3.2501 - acc: 0.4013 - top_k_categorical_accuracy: 0.5916
665600/800000 [=======================>......] - ETA: 27:56 - loss: 3.2405 - acc: 0.4028 - top_k_categorical_accuracy: 0.5931
678400/800000 [========================>.....] - ETA: 25:17 - loss: 3.2310 - acc: 0.4042 - top_k_categorical_accuracy: 0.5944
691200/800000 [========================>.....] - ETA: 22:37 - loss: 3.2220 - acc: 0.4057 - top_k_categorical_accuracy: 0.5958
704000/800000 [=========================>....] - ETA: 19:57 - loss: 3.2129 - acc: 0.4070 - top_k_categorical_accuracy: 0.5972
716800/800000 [=========================>....] - ETA: 17:17 - loss: 3.2042 - acc: 0.4083 - top_k_categorical_accuracy: 0.5985
729600/800000 [==========================>...] - ETA: 14:37 - loss: 3.1959 - acc: 0.4096 - top_k_categorical_accuracy: 0.5998
742400/800000 [==========================>...] - ETA: 11:57 - loss: 3.1875 - acc: 0.4109 - top_k_categorical_accuracy: 0.6011
755200/800000 [===========================>..] - ETA: 9:18 - loss: 3.1787 - acc: 0.4123 - top_k_categorical_accuracy: 0.6024
768000/800000 [===========================>..] - ETA: 6:38 - loss: 3.1708 - acc: 0.4135 - top_k_categorical_accuracy: 0.6036
780800/800000 [============================>.] - ETA: 3:59 - loss: 3.1623 - acc: 0.4148 - top_k_categorical_accuracy: 0.6048
793600/800000 [============================>.] - ETA: 1:19 - loss: 3.1533 - acc: 0.4162 - top_k_categorical_accuracy: 0.6061

Epoch 00002: val_acc improved from 0.38392 to 0.55859, saving model to models/CNN-multi-channel-2-epochs-1000000-rows-300-dim-2019-04-15-19:11:09-297146.h5

End: Tue Apr 16 01:03:26 EDT 2019

Start: Tue Apr 16 21:35:37 EDT 2019
model MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True)
precision@1 = 0.59619
precision@3 = 0.745185
precision@5 = 0.796235
End: Tue Apr 16 21:49:46 EDT 2019


Start: Wed Apr 17 16:27:13 EDT 2019
model MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True)
DATASET_SIZE: 100
experiment_name: 100-rows-300-dim
precision@1 = 0.0
precision@3 = 0.1
precision@5 = 0.1
End: Wed Apr 17 16:27:33 EDT 2019


Start: Wed Apr 17 16:34:41 EDT 2019
model MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True)
DATASET_SIZE: 1000000
experiment_name: 1M-rows-10-epochs
precision@1 = 0.73614
precision@3 = 0.85937
precision@5 = 0.893805
End: Wed Apr 17 16:46:36 EDT 2019

Start: Wed Apr 17 17:24:53 EDT 2019
model MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True)
DATASET_SIZE: 1000000
experiment_name: 1M-rows-10-epochs
VALIDATION_FRACTION: 0.2
validation_set_size: 180000
TRAINING_FRACTION: 1.0
training_set_size: 720000
Validation set: precision@1 = 0.7369111111111111
precision@3 = 0.8589166666666667
precision@5 = 0.8930944444444444
Test set: precision@1 = 0.73543
precision@3 = 0.85908
precision@5 = 0.89345
End: Wed Apr 17 17:36:54 EDT 2019

Start: Wed Apr 17 18:12:17 EDT 2019
NB_ALPHA: 0.1
model MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True)
DATASET_SIZE: 1000000
experiment_name: 1M-rows-10-epochs
VALIDATION_FRACTION: 0.2
validation_set_size: 180000
training_fraction: 0.025
training_set_size: 18000


Validation set:
precision@1 = 0.40434444444444445
precision@3 = 0.5348388888888889
precision@5 = 0.5882444444444445
Test set:
precision@1 = 0.40342
precision@3 = 0.53485
precision@5 = 0.58738
VALIDATION_FRACTION: 0.2
validation_set_size: 180000
training_fraction: 0.05
training_set_size: 36000


Validation set:
precision@1 = 0.5332111111111111
precision@3 = 0.6733666666666667
precision@5 = 0.7244722222222222
Test set:
precision@1 = 0.53169
precision@3 = 0.67255
precision@5 = 0.72416
VALIDATION_FRACTION: 0.2
validation_set_size: 180000
training_fraction: 0.075
training_set_size: 54000


Validation set:
precision@1 = 0.5837222222222223
precision@3 = 0.7267888888888889
precision@5 = 0.7764111111111112
Test set:
precision@1 = 0.58264
precision@3 = 0.72448
precision@5 = 0.77479
VALIDATION_FRACTION: 0.2
validation_set_size: 180000
training_fraction: 0.1
training_set_size: 72000


Validation set:
precision@1 = 0.61355
precision@3 = 0.7551611111111111
precision@5 = 0.8024055555555556
Test set:
precision@1 = 0.61228
precision@3 = 0.75441
precision@5 = 0.80193
VALIDATION_FRACTION: 0.2
validation_set_size: 180000
training_fraction: 0.15
training_set_size: 108000


Validation set:
precision@1 = 0.6529777777777778
precision@3 = 0.7900166666666667
precision@5 = 0.8334
Test set:
precision@1 = 0.65067
precision@3 = 0.78825
precision@5 = 0.83226
VALIDATION_FRACTION: 0.2
validation_set_size: 180000
training_fraction: 0.2
training_set_size: 144000


Validation set:
precision@1 = 0.6718388888888889
precision@3 = 0.8065777777777777
precision@5 = 0.8481388888888889
Test set:
precision@1 = 0.67014
precision@3 = 0.80519
precision@5 = 0.84762
VALIDATION_FRACTION: 0.2
validation_set_size: 180000
training_fraction: 1.0
training_set_size: 720000


Validation set:
precision@1 = 0.7369111111111111
precision@3 = 0.8589166666666667
precision@5 = 0.8930944444444444
Test set:
precision@1 = 0.73543
precision@3 = 0.85908
precision@5 = 0.89345
End: Wed Apr 17 18:38:20 EDT 2019

Start: Wed Apr 17 18:46:47 EDT 2019
NB_ALPHA: 0.5
model MultinomialNB(alpha=0.5, class_prior=None, fit_prior=True)
DATASET_SIZE: 1000000
experiment_name: 1M-rows-10-epochs
VALIDATION_FRACTION: 0.2
validation_set_size: 180000
training_fraction: 0.025
training_set_size: 18000


Validation set:
precision@1 = 0.30775
precision@3 = 0.4091444444444444
precision@5 = 0.4515222222222222
Test set:
precision@1 = 0.3065
precision@3 = 0.40779
precision@5 = 0.45109
VALIDATION_FRACTION: 0.2
validation_set_size: 180000
training_fraction: 0.05
training_set_size: 36000


Validation set:
precision@1 = 0.4794833333333333
precision@3 = 0.6130277777777777
precision@5 = 0.6621666666666667
Test set:
precision@1 = 0.47762
precision@3 = 0.61261
precision@5 = 0.66197
VALIDATION_FRACTION: 0.2
validation_set_size: 180000
training_fraction: 0.075
training_set_size: 54000


Validation set:
precision@1 = 0.5464722222222222
precision@3 = 0.6911166666666667
precision@5 = 0.7416666666666667
Test set:
precision@1 = 0.54765
precision@3 = 0.69051
precision@5 = 0.74036
VALIDATION_FRACTION: 0.2
validation_set_size: 180000
training_fraction: 0.1
training_set_size: 72000


Validation set:
precision@1 = 0.5863388888888889
precision@3 = 0.7313777777777778
precision@5 = 0.7799277777777778
Test set:
precision@1 = 0.58522
precision@3 = 0.72986
precision@5 = 0.77972
VALIDATION_FRACTION: 0.2
validation_set_size: 180000
training_fraction: 0.15
training_set_size: 108000


Validation set:
precision@1 = 0.6372
precision@3 = 0.7759611111111111
precision@5 = 0.81995
Test set:
precision@1 = 0.63483
precision@3 = 0.77436
precision@5 = 0.81781
VALIDATION_FRACTION: 0.2
validation_set_size: 180000
training_fraction: 0.2
training_set_size: 144000


Validation set:
precision@1 = 0.6578944444444444
precision@3 = 0.7944055555555556
precision@5 = 0.8373444444444444
Test set:
precision@1 = 0.65587
precision@3 = 0.79249
precision@5 = 0.83598
VALIDATION_FRACTION: 0.2
validation_set_size: 180000
training_fraction: 1.0
training_set_size: 720000


Validation set:
precision@1 = 0.7323388888888889
precision@3 = 0.8555611111111111
precision@5 = 0.8903277777777778
Test set:
precision@1 = 0.73014
precision@3 = 0.85548
precision@5 = 0.89047
End: Wed Apr 17 19:12:14 EDT 2019


Start: Wed Apr 17 19:19:29 EDT 2019
NB_ALPHA: 0.9
model MultinomialNB(alpha=0.9, class_prior=None, fit_prior=True)
DATASET_SIZE: 1000000
experiment_name: 1M-rows-10-epochs
VALIDATION_FRACTION: 0.2
validation_set_size: 180000
training_fraction: 0.025
training_set_size: 18000


Validation set:
precision@1 = 0.23647222222222222
precision@3 = 0.3129
precision@5 = 0.34694444444444444
Test set:
precision@1 = 0.23556
precision@3 = 0.31167
precision@5 = 0.34632
VALIDATION_FRACTION: 0.2
validation_set_size: 180000
training_fraction: 0.05
training_set_size: 36000


Validation set:
precision@1 = 0.42851666666666666
precision@3 = 0.5550833333333334
precision@5 = 0.6041888888888889
Test set:
precision@1 = 0.42691
precision@3 = 0.55253
precision@5 = 0.60275
VALIDATION_FRACTION: 0.2
validation_set_size: 180000
training_fraction: 0.075
training_set_size: 54000


Validation set:
precision@1 = 0.5139666666666667
precision@3 = 0.6559777777777778
precision@5 = 0.7061111111111111
Test set:
precision@1 = 0.5158
precision@3 = 0.65637
precision@5 = 0.70568
VALIDATION_FRACTION: 0.2
validation_set_size: 180000
training_fraction: 0.1
training_set_size: 72000


Validation set:
precision@1 = 0.5609833333333333
precision@3 = 0.7080666666666666
precision@5 = 0.7578277777777778
Test set:
precision@1 = 0.56097
precision@3 = 0.70634
precision@5 = 0.75622
VALIDATION_FRACTION: 0.2
validation_set_size: 180000
training_fraction: 0.15
training_set_size: 108000


Validation set:
precision@1 = 0.6220777777777777
precision@3 = 0.7623222222222222
precision@5 = 0.8072
Test set:
precision@1 = 0.6207
precision@3 = 0.76096
precision@5 = 0.80537
VALIDATION_FRACTION: 0.2
validation_set_size: 180000
training_fraction: 0.2
training_set_size: 144000


Validation set:
precision@1 = 0.6450166666666667
precision@3 = 0.7822888888888889
precision@5 = 0.8268
Test set:
precision@1 = 0.64345
precision@3 = 0.78168
precision@5 = 0.82583
VALIDATION_FRACTION: 0.2
validation_set_size: 180000
training_fraction: 1.0
training_set_size: 720000


Validation set:
precision@1 = 0.7273833333333334
precision@3 = 0.8522222222222222
precision@5 = 0.88685
Test set:
precision@1 = 0.72412
precision@3 = 0.85185
precision@5 = 0.8873
End: Wed Apr 17 19:45:24 EDT 2019


Start: Wed Apr 17 20:11:03 EDT 2019
NB_ALPHA: 1.0
model MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)
DATASET_SIZE: 1000000
experiment_name: 1M-rows-10-epochs
VALIDATION_FRACTION: 0.2
validation_set_size: 180000
training_fraction: 0.025
training_set_size: 18000


Validation set:
precision@1 = 0.22236111111111112
precision@3 = 0.2931111111111111
precision@5 = 0.32576666666666665
Test set:
precision@1 = 0.22091
precision@3 = 0.29174
precision@5 = 0.3248
VALIDATION_FRACTION: 0.2
validation_set_size: 180000
training_fraction: 0.05
training_set_size: 36000


Validation set:
precision@1 = 0.4163
precision@3 = 0.5411833333333333
precision@5 = 0.5904055555555555
Test set:
precision@1 = 0.41461
precision@3 = 0.53823
precision@5 = 0.58818
VALIDATION_FRACTION: 0.2
validation_set_size: 180000
training_fraction: 0.075
training_set_size: 54000


Validation set:
precision@1 = 0.5063666666666666
precision@3 = 0.6469111111111111
precision@5 = 0.6972277777777778
Test set:
precision@1 = 0.50831
precision@3 = 0.64785
precision@5 = 0.69715
VALIDATION_FRACTION: 0.2
validation_set_size: 180000
training_fraction: 0.1
training_set_size: 72000


Validation set:
precision@1 = 0.5549055555555555
precision@3 = 0.7025944444444444
precision@5 = 0.7521888888888889
Test set:
precision@1 = 0.55496
precision@3 = 0.70076
precision@5 = 0.75063
VALIDATION_FRACTION: 0.2
validation_set_size: 180000
training_fraction: 0.15
training_set_size: 108000


Validation set:
precision@1 = 0.6186555555555555
precision@3 = 0.7592666666666666
precision@5 = 0.8040222222222222
Test set:
precision@1 = 0.61745
precision@3 = 0.75767
precision@5 = 0.80237
VALIDATION_FRACTION: 0.2
validation_set_size: 180000
training_fraction: 0.2
training_set_size: 144000


Validation set:
precision@1 = 0.6420944444444444
precision@3 = 0.7796277777777778
precision@5 = 0.8242833333333334
Test set:
precision@1 = 0.64034
precision@3 = 0.77904
precision@5 = 0.82328
VALIDATION_FRACTION: 0.2
validation_set_size: 180000
training_fraction: 1.0
training_set_size: 720000


Validation set:
precision@1 = 0.7262444444444445
precision@3 = 0.8514277777777778
precision@5 = 0.8864833333333333
Test set:
precision@1 = 0.72284
precision@3 = 0.85063
precision@5 = 0.8867
End: Wed Apr 17 20:37:03 EDT 2019

